<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[详细设计-使用交互图和设计类图实现用例]]></title>
    <url>%2F2018%2F06%2F25%2F%E8%AF%A6%E7%BB%86%E8%AE%BE%E8%AE%A1-%E4%BD%BF%E7%94%A8%E4%BA%A4%E4%BA%92%E5%9B%BE%E5%92%8C%E8%AE%BE%E8%AE%A1%E7%B1%BB%E5%9B%BE%E5%AE%9E%E7%8E%B0%E7%94%A8%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[1. 设计的核心 以业务场景为中心 减少对具体语言实现和技术依赖 2. 详细设计的方法 - BCE 模型2.1 准备 用例 界面原型 与 外部系统接口文档 领域模型 与 数据库设计（ER 逻辑模型） 逻辑架构 与 应用程序框架 2.2 识别类 Boundary：与外部 Actor 交互的类。包括 UI、外部系统接口 Controller：处理外部事件，实现控制流的类。通常是一个子系统、一个用例一个类 Entity：领域对象或数据实体 其中，UI 类识别比较难，依赖实现技术。传统技术是 window 表示一个 UI，随着 HTML 等多媒体页面表示工具的出现，不同功能的内容在一个界面中共存。 2.3 动态图设计 UI类（如业务表单）放最左边，控制器对象放置中间，实体放置右边，外部系统放在最右边 使用构造型细化四种类 Boundary，Controller，Entity，Interface/Boundary 按 BCE 规则实现用例中主要场景 Boundary 只能和 Controller 联系，Controller 处理 UI 事件 Entity 只能和 Controller 联系，Controller 请求 Entity 加工自己或关联的数据 即 Entity 提供业务服务，包括简单的 CRUD 资源操作 按 UI 事件顺序，描述 UI 事件被处理的过程 2.4 静态设计 Boundary 对象（完善界面不在讨论范围内） Controller 对象 Boundary 发生的用户事件消息，皆是 Controller 的方法。 以下都是不正确的交互（严格的层次模型）： UI 有箭头指向模型（注意 ViewModel 与 Model 的区别） 模型有箭头指向控制器。或控制器有除创建之外的箭头指向界面 无论安卓或web，控制器都设计为多用户。即控制器不包含状态变量 不能考虑多线程，使用多线程更新界面。要使用回调函数（消息）机制完成异步操作 Entity 对象 从 领域模型 获取属性 如果模型之间存在关联，请将关系转化为合适的实现（关联属性） 将 Controller 消息转化为方法 2.5 映射不同架构和框架映射机制不一样，以传统 java web 为例： 表示层 M （ViewModel） 与 用例涉及的 Entities 数据一致， 放入 models 或 pojos 或 entities 包 V （View） 就是视图模板，或部分视图模板（如查询表单） C （Controller） 与 Controller 对象一致，处理一类 UI 事件 如果模板数据 是 Entities 数据的投影、join，设计为 dto（data transfer object） 对象，放入 dtos 包 如果一个表达或数据需要在多个界面共享，可设计为应用程序范围或 Session 范围变量，如输入表单，一般放入 form 包 将常用数据验证方法、翻页等，应写成统一的实用程序包 utilities 将常用数据转换（序列化、反序列化、格式化）类，放在 converter 包中 业务层（services 包） Entities 的方法 获取关联对象的方法 数据层（daos 或 repos 包） Entities 的 CRUD 方法 3. make reservation 用例的详细设计 使用 ECB 实现 make reservation 用例的详细设计（包含用例简介，顺序图，类图） 3.1 用例简介用例：make reservation，选择限制查找酒店，选择酒店，选择房间和入住日期，确认预订酒店订单 用例图： 3.2 识别类 Boundary Controller Entity 1. 酒店预订搜索页面(SearchPage)2. 酒店房间预订页面(ReservationPage) 1. 后台酒店管理服务(FindHotel)2. 后台订单管理服务(CreateReservation) 1. 酒店信息(Hotel)2. 预订订单(Reservation)3. 地点(Location) 3.3 顺序图 3.4 类图 将逻辑设计类图映射到实际项目框架的包图。用树形结构表述实现的包和类 3.5 树状结构]]></content>
      <categories>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>软件工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件项目架构与框架]]></title>
    <url>%2F2018%2F06%2F05%2F%E8%BD%AF%E4%BB%B6%E9%A1%B9%E7%9B%AE%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[本文主要探讨Web应用的多层架构 1. 软件架构与框架之间的区别与联系1.1 概念解释1.1.1 架构 (Architecture) An architecture is the set of significant decisions about the organization of a software system, which describe the selection of the structural elements and their interfaces by which the system is composed，and their behavior as specified in the collaborations among those elements 软件架构描述一个系统的组成元素，元素之间的接口，以及各个元素之间的协作行为，即软件系统的组织架构。 1.1.2 架构模式 (Architecture Styles) An architectural style guides the organization of these elements and their collaborations to solve common problems of the specific domain. 软件架构模式是特定领域的组织解决方案，可以说是架构的特定风格，如信息系统领域的经典三层架构 三层架构： 表示层（Presentation Layer） Models，Views，Controllers State Management 业务层（Business Layer）/服务层（Service Layer） Transacton（交易） Query（查询） 数据持久化层（Persistent Layer）/ 数据访问层（Data Access Layer/DAO） CRUD（实体的四种操作） ORM（实体与关系映射） 关于三层架构与MVC：三层架构可以应用于任何语言、任何技术的应用程序；而MVC只是为了解决BS应用程序表现层各部分的耦合关系。它们互不冲突，可以同时存在，也可根据情况使用其中一种。MVC 关注的重点在于表现层的代码组织方式，通过降低代码间的耦合度，使代码更改维护。 总之：三层架构是一种宏观的架构模式，应用于整个软件项目。MVC是一种架构模式，一般可以灵活的运用于项目的部分（如前端）。一种架构模式往往使用了多种设计模式 详情见：前端开发中的 MVC/MVP/MVVM 模式，MVC模式与三层架构的区别 1.1.3 应用框架 (Application Frameworks) 应用框架是特定语言和应用的架构解决方案。 框架是面向某领域（包括业务领域，如ERP，和计算领域，如GUI）的、可复用的“半成品”软件，它实现了该领域的共性部分，并提供一系列定义良好的可变点以保证灵活性和可扩展性 1.2 架构模式与框架的区别与联系 区别：框架是一种软件，是由特定语言实现，并应用于特定领域的软件。架构模式不是软件，架构模式是一种思想、风格，是一个抽象的概念。 联系：框架是架构模式的解决方案。架构模式是框架应用时的组织思想。 2. 一个架构模式例子 我们即将描述的系统架构是我的一个Web项目：知单(Zhidan)智慧餐厅系统 我们的项目采用的是经典的三层架构模式 3. 研究 VUE 与 Flux 状态管理的异同3.1 概念解释3.1.1 Flux简单说，Flux 是一种架构思想，专门解决软件的结构问题。它跟MVC 架构是同一类东西，但是更加简单和清晰。Flux的核心理念是单向数据流。Flux应用有三个主要部分：Dispatcher调度 、存储Store和视图View(React 组件)。 在这种架构当中，Views 查询 Stores（而不是 Models），并且用户交互将会触发 Actions，Actions 则会被提交到一个集中的 Dispatcher 当中。当 Actions 被派发之后，Stores 将会随之更新自己并且通知 Views 进行修改。这些 Store 当中的修改会进一步促使 Views 查询新的数据。 3.1.2 VueVue是一种前端框架。Vue.js是一套构建用户界面(user interface)的渐进式框架。 Vuex：Vue的状态管理工具 3.2 Vue与Flux状态管理的异同 Flux是一种架构。Flux主要组成部分是Dispatcher，Store，View，Action。 Vue状态管理Vuex是一种工具，Vue中的VueComponent相当于Flux的View。而状态管理工具Vuex主要组成部分是State，Mutations，Action，其中Mutations+State相当于Flux的Store部分。Vuex中还去除了Dispatcher部分。]]></content>
      <categories>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>软件工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dual-Glance模型]]></title>
    <url>%2F2018%2F05%2F29%2FDual-Glance%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[对Dual-Glance Model for Deciphering Social Relationships论文的学习，这篇论文研究使用深度学习的方法识别图像中一对人的社会关系 一、PISC数据集 PISC数据集是Dual-Glance这篇论文中为了训练以及测试社会类型检测网络而收集制作的数据集，来源一部分是网上社交平台收集图片，一部分是来自COCO数据集。下面我们只介绍六种关系的标注信息。 数据集标注包含两个文件：relationship.json，annotation_image_info.json annotation_image_info.json：文件中的每一项标注图片中的对应bbox和图片来源等信息（下面只举例包含bbox信息的一个实例） 例子： 12345678910[ &#123; 'id': 3002, //image id: 3002，注意这里id是int形式，和下面relationship的id是一样的，只是下面的用了五位字符串'3002' 'bbox': [ [33, 22, 111, 111], // bounding box 代表box的四个边的位置[left, top, right, down] [13, 12, 211, 211], [53, 52, 311, 311] ] &#125;] relationship.json：文件中的每一项标注是由图片id和对应图片中的关系pair组成 关系用{1: friends, 2: family, 3: couple, 4: professional, 5: commercial, 7: no relation}}，对应的关系标号表示 例子： 123456789101112&#123; '3002': &#123; // image id: 3002 '1 2': 1, // 图片中第一个bbox和第二个bbox组成pair，这对pair的关系是1 '2 3': 1, // 图片中第二个bbox和第三个bbox组成pair，这对pair的关系是1 '3 1': 0 // 图片中第三个bbox和第一个bbox组成pair，这对pair的关系是0 &#125;, '3001': &#123; // image id: 03001 '1 2': 1, // 图片中第一个bbox和第二个bbox组成pair，这对pair的关系是1 '2 3': 1, // 图片中第二个bbox和第三个bbox组成pair，这对pair的关系是1 '3 1': 0 // 图片中第三个bbox和第一个bbox组成pair，这对pair的关系是0 &#125;&#125; 数据集包含三类图片索引，分别是：训练集，测试集，验证集（如：测试集relation_trainids.json）。每个索引文件都是用图片id表示对应图片 例子： 1234[ &apos;3001&apos;, &apos;3002&apos;] 数据集图片：图片格式为jpg，图片命名为五位数字。如果id不足五位，id前面补0。如：id: 12 =&gt; image: 00012.jpg 二、Dual-Glance模型先上模型图： 模型一共由两个大的网络组成，这两网络分别为First Glance， Attentive RCNN for Second Glance（分别对应图中的上下两个网络）； 2.1 First Glance 论文分别提取一对pair的bbox（p1 p2：patch1 patch2）和pair的联合bbox（pu：patch union） 我们再提取pair的标注信息$b1^{pos}, b_2^{pos}$ ( $b^{pos} = {x{min}, y{min}, x{max,} y{max}, area{i}} ∈ \R^5 $ )，将两个向量concat起来得到一个特征向量b 在提取1，2中的数据后，我们使用ResNet（预先pre-trained经过 ImageNet classification task），分别作用于resize到224x224的patches（pu, p1, p2）。 其中一个关键点是，三个patches经过CNN（ResNet）网络的最后一层卷积层之后，不进入原先ResNet全连接层，而是做flatten。最后我们得到从ResNet最后一层卷积层输出2048-d图像特征向量； 最后注意：如上图所示，p1和p2的CNN是用的shared weight的方式，表示训练于同一个CNN网络。` 在处理p1, p2, pu的同时，我们也要处理b向量。我们这里的处理方法是将b输入一个全连接层，输出256-d特征向量。 最后将3，4中的特征向量concat到一起得到一个超长的特征向量6400-d: [256-d, 2048-d, 2048-d, 2048-d]，输入下一个全连接层，得到4096-d的输出，接着4096-d进入最后一个全连接层，得到6-d的scores s1。 这样就完成First Glance的全部forward流程。 2.2 Second Glance Second Glance的提出是为了实现attention mechanism（注意力机制）引导。 用图片中的cue上下文信息结合pair信息做学习，提高模型准确性 原图通过一个RPN网络，被提取出RoI区域（这些RoI还要经过非最大化抑制，被过滤出IoU合适的RoI） 我们将原图输入一个CNN网络（这里用VGG实现），得到feature map；我们再从feature map中映射出RoI对应区域，以减小不必要的CNN计算。最终，在这一步，我们得到一系列RoI的feature map。 我们再将每个RoI投入一个RoI pooling层，使得各个RoI大小一致，以便进入下一个全连接层。每个RoI feature map进入全连接层，被转化为一个4096-d的feature vector $v_i$。 在我们完成3后，我们在这一步开始用到first glance中间的一个4096-d的feature vector $v{top}$。我们先将两者通过一个weight（trainable）的参数$w{top}$连接起来，通过下面的公式： h_i = v_i + w_{top} \otimes v_{top}公式中的$\otimes$表示点乘(element-wise multi)。 我们接着用$h_i$ 计算attention $a_i \in [0, 1]$。如下，一个LR模型将$h_i$映射到$[0, 1]$ a_i = \frac 1 {1 + exp(-(W_{h, a} h_i + b_a))}$W_{h, a} \in \R^{1 \times k}$是一个矩阵， $b_a \in \R $是偏置 我们将这个$a_i \in [0, 1]$乘入$v_i$中，得到一个新的feature vector $v_i^{att}$ ，最后将$v_i^{att}$输入一个全连接层，得到每个RoI的score： $ s_i = W_s v_i^{att} + b_s$ 最后，我们将$si$输入一个lse函数，得到$S_2 = log[1 + \sum{i=1}^Nexp(S_i)]$ 2.3 联合$S_1, S_2$我们设置一个权重将最终的$S_1, S_2$联合起来： S=S_1(I, b_1, b_2) + \alpha S_2(I, b_1, b_2, R)这是各个类别的最终得分，再通过softmax转化为各个类别的检测概率： p_r = \frac {exp(S_r)} {\sum_r exp(S_r)}]]></content>
      <categories>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>social relationship</tag>
        <tag>Faster R-CNN</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑斯蒂回归模型]]></title>
    <url>%2F2018%2F05%2F28%2F%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[逻辑斯蒂分布函数：$F(X\leq x) = \frac 1 {1 + e^{\frac {x-\mu } {\gamma} }}$ 一、二项逻辑斯蒂回归模型1.1 概率角度二项逻辑斯蒂回归模型是如下条件概率分布： P(Y=0| x) = \frac 1 {1 + exp(w·x + b)} \\ P(Y=1| x) = \frac {w·x + b} {1 + exp(w·x+b)} \\ x \in R^n ,\ Y \in \{0, 1\} ,\ w \in R^n ,\ b \in R这里的$x$是输入，$Y$是输出，$w, b$是训练变量 1.2 回归角度 普通的线性模型适用于线性回归问题 当遇到分类问题时，相当于线性回归的输出值域变为${0, 1}$，所以单位阶跃函数可以说是一个理想的选择，但是这个函数并不是可导的，所以在求解最优模型的过程中，我们无法使用梯度下降的方法 所以就用一个形状类似”S”的函数来近似单位阶跃函数，用这个函数将值域映射到$[0, 1]$，这是个连续的区间。 所以就使用了对数几率函数$g(z) = \frac 1 {1+e^{-z}}$ 二项逻辑斯蒂回归模型是一个预测样本$x$标签$y\in[0,1]$的函数，弥补了线性函数在二分类问题上的不足，将原本$w·x+b \in R$的线性模型值域，映射到$[0, 1]$这个值域范围： h_\theta(x) = g(\theta^T x) ,\ \text {这里x是齐次坐标，} \theta^T x \text {相当于} w·x+b\\ g(z) = \frac 1 {1+e^{-z}}所以有：$h_{\theta}(x) = \frac 1 {1 + e ^{- \theta^Tx}}$ 转化为线性回归问题： 曾经尝试过通过解析几何的方式，解逻辑斯蒂回归模型解析解 令$h_\theta (x) = y$ ，则可以得到 $ ln \frac y {1-y} = \theta^T x$ 这个形式，这样就可以列矩阵方程组解$\theta$的值，其中需要将标注由0 映射为0.0001， 由1映射为0.9999，由于分母不能为零，相当于把标签化为概率。实际上这样操作得到的精度不高 对数几率：上面出现的 $ln \frac y {1-y}$，如果将y视为x为正样本的概率，则$\frac y {1-y}$就是几率，而$ln \frac y {1-y}$就是对数几率也叫logit。从这个角度看，也是相当于把原本二值标签通过对数几率映射到$R$ ，使得用线性回归拟合对数几率。（所以是对对数几率的回归，也叫logit regression） 二、模型选择的策略 上面的内容提出了逻辑斯蒂回归模型，对于同一个逻辑斯蒂回归模型，有无数种可能的$w, b, (\theta)$ ，所以要对模型$h_{w_i, b_i}$ 做出评价，评价这个模型的好坏，才能进入模型的学习阶段。这个评价的准则，就是一种模型选择的策略 通过把模型输出$y$看作是正样本的概率，如1.1中的模型角度。为解这个概率模型，我们选择”极大似然法“，对模型中参数评价好坏。 模型中，每个样本$xi$的正样本概率：$p(y_i = 1 | x_i) = h\theta(xi)$ ，负样本概率$p(y_i = 0| x_i) = 1-h\theta(x_i)$ 所以极大似然估计，模型中参数为$w, b$的可能性为：$l(w, b) = \prod [y·p(y_i = 1| x_i) + (1-y)·p(y_i = 0 | x_i)]$ ，其中$y·p(y_i = 1| x_i) + (1-y)·p(y_i = 0 | x_i)$是似然项。一般会化成对数似然函数求解$max\ l(w, b)$ ： l(w, b) = \sum log(y·p(y_i = 1| x_i) + (1-y)·p(y_i = 0 | x_i))所以这个对数似然函数即为我们的策略：$l(w, b)$，希望最大化对数似然函数 损失函数： 在通常情况下，我们偏向于将模型选择的策略视为一个模型的损失函数，并且希望这个损失函数越小越好。所以为了上述对数似然函数安装损失函数的方式理解，我们加入负号：把最大化对数似然函数任务转换成最小化损失函数$l(w, b) =-\sum log(y·p(y_i = 1| x_i) + (1-y)·p(y_i = 0|x_i))$ py老师的机器学习课程上，将其表述为：$J(\theta) = - \frac 1 m \sum{i=1}^m [y^{(i)}log h\theta(x^{(i)}) + (1-y^{(i)} log (1 - h_\theta(x^{(i)})))$ ，本质上是一样的 三、学习的算法 有了模型（假设空间），有了策略（模型评估），现在需要的就是用一套算法去学习出评价最优的模型 通过上述两点的总结，这里我们把模型的学习算法目标归结为最小化损失函数的最优化问题 按照最优化的观点看，目标函数（损失函数）是凸函数，因此多种优化算法都能够适用，保证可以找到全局最优解。常见算法有：梯度下降法（SGD）、迭代尺度法（IIS）、牛顿法（BGFS）或拟牛顿法（L-BGFS）。牛顿法或拟牛顿法一般收敛速度更快。 这里我们探讨梯度下降算法，核心公式： \theta_{i+1} = \theta_i - \alpha \frac {\partial J(\theta)} {\partial \theta_j}难度不大，只需要对$J(\theta)$求各维参数的导数即可： \frac {\partial J(\theta)} {\partial \theta_j} = \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}四、正则项 引入正则化项的目的是为了防止过拟合问题。 根据奥卡姆剃刀原则，解决同样的问题的模型中，尽量选择简单的模型 正则化项就是为了降低LR模型中系数的复杂度 解决过拟合的两种方法： 降低特征的数量 手工筛选特征 模型选择算法 正则化 保留特征，但是减小特征的大小 效果很好，如果我们有很多特征，保留每个特征都能为预测y做贡献 4.1 为$J(\theta)$引入正则化项 这样的正则化项迫使得模型的参数$\theta$尽可能的小。为此我们还需要重新求导，重新计算梯度公式：]]></content>
      <categories>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>逻辑斯蒂回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[需求分析建模练习]]></title>
    <url>%2F2018%2F05%2F14%2F%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90%E5%BB%BA%E6%A8%A1%E7%BB%83%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[我根据这份说明：Airbnb需求说明，进行了五种UML建模练习（用例图、用例（业务）活动图、领域建模、状态建模、系统顺序图），建模结果如下。 Airbnb民宿预订系统用例图 Airbnb民宿预订系统确认支付用例活动图 Airbnb领域模型 Airbnb民宿订单状态图 Airbnb确认并支付场景系统顺序图]]></content>
      <categories>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>练习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Web应用安全认证机制]]></title>
    <url>%2F2018%2F05%2F08%2FWeb%E5%BA%94%E7%94%A8%E5%AE%89%E5%85%A8%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[引言之前对Session/Cookie机制做了仔细的研究：Session/Cookie的提出是为了解决HTTP无状态的问题，为了识别客户端请求是来自哪个客户，同时也记住该客户的状态。 而在最近做项目的过程中发现一个新的概念：Authentication，即认证的概念。之前对这个概念不是很有兴趣去了解，由于一直把OAuth2.0当作是第三方应用接入Web后台api的认证标准，所以觉得自己可能不需要这方面的学习。但是最近的后端项目中，我仔细研究了RESTful的设计规范之后，发现其实Web应用的认证是很有必要按照一定的权威标准去做的，所以写下了这篇学习总结。 下面的内容就几种常见的Auth机制做介绍，各个方式的适用场景都是互不相同的。 1. BA (Basic access authentication) wiki上名词解释：https://en.wikipedia.org/wiki/Basic_access_authentication 这是客户端代理的一种方法，每次请求都会带上用户名和密码 特点是不需要cookie，session，或者login的页面。HTTP头部域有专门区域BA field，用于控制BA。 简单的说就是每次请求都需要加入用户名和密码，每次请求都需要访问数据库验证信息。所以这种方式是效率极低，又不安全的做法。 在Swagger2.0（一种API框架）中，提供了BA的API书写方式https://swagger.io/docs/specification/2-0/authentication/basic-authentication/ 2. Cookie/Session Auth这种认证机制是通过在第一次请求认证的时候，在服务端添加Session，在客户端保留Cookie，之后每次请求通过Cookie信息和Session信息匹配的方式管理状态。 3. OAuth OAuth（开放授权）是一个开放的授权标准，允许用户让第三方应用访问该用户在某一web服务上存储的私密的资源（如照片，视频，联系人列表），而无需将用户名和密码提供给第三方应用。 OAuth允许用户提供一个令牌，而不是用户名和密码来访问他们存放在特定服务提供者的数据。每一个令牌授权一个特定的第三方系统（例如，视频编辑网站)在特定的时段（例如，接下来的2小时内）内访问特定的资源（例如仅仅是某一相册中的视频）。这样，OAuth让用户可以授权第三方网站访问他们存储在另外服务提供者的某些特定信息，而非所有内容 这种基于OAuth的认证机制适用于个人消费者类的互联网产品，如社交类APP等应用，但是不太适合拥有自有认证权限管理的企业应用； 4. JWT (Json Web Tokens)JWT是一种规范，这个规范允许我们使用JWT在客户端和服务器之间传递安全可靠信息。 对于Token Auth，Swagger2.0有相关编写规范：https://swagger.io/docs/specification/2-0/authentication/api-keys/ 4.1 JWT组成一个JWT实际上就是一个字符串，它由三部分组成，头部、载荷与签名。 载荷 Payload 这个载荷就是相当于将一部分信息（可以是Session信息）用json表示，这部分的内容的字段在JWT标准中可以找到： 123456789&#123; &quot;iss&quot;: &quot;John Wu JWT&quot;, &quot;iat&quot;: 1441593502, &quot;exp&quot;: 1441594722, &quot;aud&quot;: &quot;www.example.com&quot;, &quot;sub&quot;: &quot;jrocket@example.com&quot;, &quot;from_user&quot;: &quot;B&quot;, &quot;target_user&quot;: &quot;A&quot;&#125; 这里面的前五个字段都是由JWT的标准所定义的。 iss: 该JWT的签发者 sub: 该JWT所面向的用户 aud: 接收该JWT的一方 exp(expires): 什么时候过期，这里是一个Unix时间戳 iat(issued at): 在什么时候签发的 然后对这部分做base64编码（这是一种编码方式，不是加密方式，可以重新恢复成原来的编码格式），得到新的字符串如下： 1eyJpc3MiOiJKb2huIFd1IEpXVCIsImlhdCI6MTQ0MTU5MzUwMiwiZXhwIjoxNDQxNTk0NzIyLCJhdWQiOiJ3d3cuZXhhbXBsZS5jb20iLCJzdWIiOiJqcm9ja2V0QGV4YW1wbGUuY29tIiwiZnJvbV91c2VyIjoiQiIsInRhcmdldF91c2VyIjoiQSJ9 头部 Header JWT的头部是描述该JWT的基本信息，如描述类型和所用签名算法: 1234&#123; &quot;typ&quot;: &quot;JWT&quot;, &quot;alg&quot;: &quot;HS256&quot;&#125; 对头部做base64编码，就可以得到头部字符串 1eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9 签名 Signature 将头部字符串和载荷字符串用’.’连接到一起得到下面字符串 1eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJmcm9tX3VzZXIiOiJCIiwidGFyZ2V0X3VzZXIiOiJBIn0 在对其做上面说的HS256签名算法做加密，我们可以为其提供一个加密的密钥，最后加密后得到的字符串就是我们的签名： 1rSWamyAYwuHCo7IFAgd1oRpSP7nzL7BF5t7ItqpKViM JWT 吧上面的合在一起就是我们的最终JWT 1头部字符串 + &apos;.&apos; + 载荷字符串 + 签名字符串 载荷字符串部分用于获取身份信息，签名部分用于验证来源是否合法。 JWT不仅可以用于身份认证，还能用于Web应用直接的通信，和单点登录的实现 4.2 JWT做身份认证 在做第一次认证的时候，将用户信息用作载荷，产生JWT，返回到客户端（用Cookie形式存储） 每次请求，检查Cookie：a. 是否过期 b. 身份信息是否合法 c. 签名是否合法。如果通过就说明身份认证成功 5. 微信小程序登录认证与授权下面是两个做了详细介绍的链接，有空会去做深入研究。 小程序登录 梳理微信小程序登录时序图：授权与 Oauth2.0 References: https://www.jianshu.com/p/88b7be4657a3 http://www.cnblogs.com/xiekeli/p/5607107.html https://swagger.io/docs/specification/2-0/authentication/ http://blog.leapoahead.com/2015/09/06/understanding-jwt/ http://blog.leapoahead.com/2015/09/07/user-authentication-with-jwt/]]></content>
      <categories>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>Authentication</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RESTful风格API理解以及设计]]></title>
    <url>%2F2018%2F05%2F08%2FRESTful%E9%A3%8E%E6%A0%BCAPI%E7%90%86%E8%A7%A3%E4%BB%A5%E5%8F%8A%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[1. 基本理解 - RESTful RESTful（representational state transfer）是一种软件架构风格，目的是便于不同的软件或程序在一个网络中互相传递资源。 resources：资源。网络上的一个资源就代表一个实体或信息；一个资源可以代表一个图片，一个文件，一种服务；每个资源都有一个URI（统一资源标识符）指向它，每个资源都对应一个特定的URI；获取某个资源的方法，即是访问这个资源的URI即可。URI最常见形式是URL（统一资源定位符） representation：表现层。资源是一种信息实体，对外可以有各种各样的形式表现，比如一段文本可以用html格式、xml格式、txt格式，这些形式就是表现层（明确的说，表现层代表资源实体的表现形式）；URI代表的是一个资源的ID，而没有包含表现层信息，所以表现层的具体表现形式应该是在HTTP请求头部的Accept和Content-Type字段，这两个字段才是对表现层的具体描述 state transfer：状态转换。客户端和服务端的交互需要改变用户状态，而HTTP协议是无状态的（即对客户端操作得到的状态是无法通过HTTP协议保留的），所以状态都保留在服务端（用户状态的实质就是服务端的数据）。 REST的名称由来 REST-compliant web services allow the requesting systems to access and manipulate textual representations of web resources by using a uniform and predefined set of stateless operations. 通过一组统一的、预先定义的无状态操作，REST风格的web服务允许请求系统（客户端）访问和操作文本（html，xml，json。。）表现形式的web资源 The term is intended to evoke an image of how a well-designed Web application behaves: it is a network of Web resources (a virtual state-machine) where the user progresses through the application by selecting links, such as /user/tom, and operations such as GET or DELETE (state transitions), resulting in the next resource (representing the next state of the application) being transferred to the user for their use. 该术语旨在唤起设计良好的Web应用程序的行为形象：它是一个Web资源（虚拟状态机）的网络。在这个网络中，用户通过选择链接（例如/ user / tom）和诸如GET或DELETE（状态转换）之类的操作在应用程序中推进，导致下一个资源（表示应用程序的下一个状态）被传送给用户以供其使用。 2. 设计规范 - RESTful Web API 现在web应用最流行的API规范就是REST风格的API，在RESTful Web应用中，实现一个业务流程需要一系列后端的资源状态的转换，每个资源都通过一个URL唯一定位，而操作这些资源的方法就是标准的HTTP方法，通过HTTP方法作用于资源的URL，实现对资源状态的转换，实现Web应用的业务。 下面内容讨论RESTful Web API的设计规范 2.1 通信协议标准HTTPS、HTTP协议 2.2 URL组成规范 根URL + API版本信息 + URL末端 2.2.1 根URL 专有API二级域名 1https://api.example.com 域名下分配一级目录 1https://example.com/api 2.2.2 API版本信息将版本信息加入根URL的下一级路径： 123https://api.example.com/v1orhttps://example.com/api/v1 2.2.3 URL末端这部分最为重要，这里的URL设计彰显你的资源的分层结构和相互关系 最终组成的URL指向的是一个资源（Resource）或者一种资源的集合（Collection），URL末尾中不应该出现动词，应该都是名词，同时如果能够返回集合的资源要使用复数形式。 举个关于用户的例子： 获取所有用户信息，末端应该设计成 `/users, 即: 1https://api.example.com/v1/users 获取用户编号001的信息，末端应该设计成 /users/001, 即: 1https://api.example.com/v1/users/001 2.3 资源操作规范 参考HTTP方法的操作 资源操作基本覆盖CRUD所有操作 HTTP的操作方法在 RESTful 中有各自的语义，理解它们的语义至为重要。 方法 语义 例子 说明 GET 选择、获取 GET /users/001 获取编号 001 用户的信息 POST 新建 POST /users 新建一个用户的信息 PUT 更新(完整) PUT /users/001 更新编号 001 用户的全部信息，客户端提供该用户的全部信息 PATCH 更新(局部) PATCH /users/001 更新编号 001 用户的部分信息，客户端只提供该用户的部分信息 DELETE 删除 DELETE /users/001 删除编号 001用户的信息 HEAD 获取(元数据) HEAD /users/001 获取编号 001 用户的元数据，如用户数据的哈希值或最后修改时间 OPTIONS 获取(权限信息) OPTIONS /users/001 获取客户端能编号 001用户进行哪些操作，即操作的权限 在GET请求中通过设置URL末尾中的查询参数，过滤资源‘ 参考Google API设计规范（Google的设计思想更贴近资源的CRUD，更符合业务理解） Standard Method HTTP Mapping HTTP Request Body HTTP Response Body List GET &lt;collection URL&gt; N/A Resource* list Get GET &lt;resource URL&gt; N/A Resource* Create POST &lt;collection URL&gt; Resource Resource* Update PUT or PATCH &lt;resource URL&gt; Resource Resource* Delete DELETE &lt;resource URL&gt; N/A google.protobuf.Empty** 2.4 状态码使用规范按照HTTP本身状态码标准，使用状态码。这里列出基本常用情况： 状态码 语义 HTTP 方法 说明 200 OK GET 成功返回用户请求的数据 201 Created POST/PUT/PATCH 用户新建或修改资源成功 202 Accepted * 成功发起一个异步任务 204 No Content DELETE 删除资源成功 400 INVALID REQUEST POST/PUT/PATCH 请求有错误，服务端没有对资源进行任何操作 401 Unauthorized * 表示用户没有权限（令牌、用户名、密码错误） 403 Forbidden * 表示用户得到授权（与401错误相对），但是访问是被禁止的 404 NOT FOUND * 请求对应的资源不存在 406 Not Acceptable GET 用户请求的格式不可得（比如用户请求JSON格式，但是只有XML格式）, 即未支持的表现层 410 Gone GET 用户请求的资源被永久删除，且不会再得到的 422 Unprocesable entity POST/PUT/PATCH 当创建一个对象时，发生一个验证错误 500 Internal Server Error * 服务器内部错误 2.5 错误响应规范如果状态码是4xx，就应该向用户返回出错信息。错误响应的结构如下（参照Google API 设计）： 1234567891011&#123; &quot;error&quot;: &#123; &quot;code&quot;: 401, &quot;message&quot;: &quot;Request had invalid credentials.&quot;, &quot;status&quot;: &quot;UNAUTHENTICATED&quot;, &quot;details&quot;: [&#123; &quot;@type&quot;: &quot;type.googleapis.com/google.rpc.RetryInfo&quot;, ... &#125;] &#125;&#125; 2.6 返回规范对于不同操作方法和操作对象(集合或个体)，服务器返回的结果应该符合以下规范。 示例 返回 GET /collection 返回资源对象的列表（数组） GET /collection/resource 返回单个资源对象 POST /collection 返回新生成的资源对象 PUT /collection/resource 返回完整的资源对象 PATCH /collection/resource 返回完整的资源对象 DELETE /collection/resource 返回一个空文档 另外，返回的数据格式(Representation)应该尽量使用JSON。 3. 设计流程如下参考Google API Design Determine what types of resources an API provides.设计API能够提供的资源类型 Determine the relationships between resources. 决定资源之间的关系 Decide the resource name schemes based on types and relationships. 决定资源的类型名基于关系和类型 Decide the resource schemas. 决定资源的纲要 Attach minimum set of methods to resources. 为每个资源添加满足需求的最少操作方法 References: Representational State Transfer 理解 RESTful Principles of good RESTful API Design Google API设计指南－面向资源的设计 RESTful HTTP in practice]]></content>
      <categories>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>RESTful</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[McCabe环路复杂度分析]]></title>
    <url>%2F2018%2F05%2F04%2FMcCabe%E7%8E%AF%E8%B7%AF%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[1. McCabe复杂度概念 McCabe 复杂度 (Thomas J. McCabe, Sr., 1976) 方法对程序流程图进行静态分析，将其转化为程序控制流图 CFG (有向图)，然后以图论的方法进行严格的结构分析，是对程序拓扑结构复杂性的度量。 以我的理解，McCabe复杂度，是程序控制流图的拓扑结构复杂度，这个过程需要将程序转换成有向图。 进行程序流图的复杂度分析的一个重要基础是流程图的可规约性reducibility；一个不使用goto的程序是可规约的，反之是不可规约的。总之进行McCabe复杂度分析要求程序满足结构化程序设计的准则。 McCabe复杂度： 环路复杂度 基本复杂度 模块设计复杂度 设计复杂度 继承复杂度 行数 规范化复杂度 全局数据复杂度 局部数据复杂度 病态数据复杂度 1.1 McCabe环路复杂度 一个程序模块的环路复杂度用来衡量模块中判定结构的复杂程度，数量上可以表现为程序控制流图中从开始点到终结点的独立路径条数，相当于合理预防错误所需测试的最少路径条数。 最大独立路径数目：独立路径集合指的是从开始点到终结点遍历路径，每次经过的路径必须包含至少一个遍历过的边，最后遍历所有边之后，得到的路径集合就是独立路径集合（集合中两两路径之间一定互相有对方没有存在的边），这样的集合不是固定的，所以这样最大的集合的路径数量就是最大独立路径数目 程序的可能错误和高的环路复杂度有着密切的关系，环路复杂度大说明程序代码可能质量低而且难以测试和维护。 McCabe环路复杂度计算公式 $V(G) = m - n + 2p$ m是G的边数目 n是G的节点数 p是G的连通分支数 其他计算技巧 简单程序流图是连通图，p=1 G是平面图时，由欧拉公式，V(G)=R。其中R是平面被控制流图划分成的区域数目 (包括外部面)。 简单的单入口单出口结构化模块， V(G) 值等于程序控制流图中的单条件判断节点的个数 +1。多条件判断条件可以先转化为单条件复合结构再应用本结论 在程序控制流图G中增加从出口指向入口的辅助边，得到一个强连通图 G’。该程序的环路复杂度定义为图 G’ 的秩数 简单的单入口单出口模块的McCabe环路复杂度等于程序控制流图的最大独立路径数目，它指出为防止出错所需要的最少测试次数，典型应用于白盒测试的基本路径测试方法 其他复杂度内容在这里不做展开 2. 实例计算McCabe环路复杂度 根据上面的程序流程图，完成: a. 画出相应的程序控制流图; b. 给出控制流图的邻接矩阵; M = \begin{bmatrix} 0 & 1 & 0 & 0 & 0 & 0\\ 0 & 0 & 1 & 1 & 0 & 0 \\ 0 & 0 & 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 0 & 0 & 0 \end{bmatrix} 从左到右，从上到下是A, B, C, D, E, F c. 计算 McCabe 环形复杂度; i. V(G) = m - n + 2p = 7 - 6 + 2 = 3 ii. V(G) = R (划分区域数) = 3 iii. V(G) = 判定点节点数+1 = 3 d. 找出程序的一个独立路径集合。 (a): A-B-D-E-F (b): A-B-D-F (c): A-B-C-D-F]]></content>
      <categories>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>McCabe复杂度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[状态建模学习]]></title>
    <url>%2F2018%2F05%2F04%2F%E7%8A%B6%E6%80%81%E5%BB%BA%E6%A8%A1%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[1. 状态建模基础 References: http://www.cnblogs.com/ywqu/archive/2009/12/17/1626043.html 1.1 状态建模概念 领域模型描述了问题域中事物及其之间的关系与量化的约束，我们需要进一步验证模型的有效性与完备性，管理这些事物的生命周期成为有效的方法 - 状态建模。它主要解决以下问题： 从实例的角度识别业务事件，完善、优化业务过程的细节，细化业务过程与领域模型 给出业务过程合理性与完备性验证 为程序开发提供业务规范细节 1.2 UML 状态建模符号体系状态图描述一个 事物或对象 受 事件或消息 刺激产生 可见的状态（属性/属性组合） 的数据变化。 基础符号 起始状态（Initial） 终止状态（Final） 取消/对象取消（Termination） 状态（State） 变迁（Transform），含条件（Condition）、事件（Event）和事件处理动作（Action/Handler） 扩展符号 复合状态 信号 绘图注意事项： 必须有起始状态，通常有终止和取消状态 状态命名要用名词短语、动词过去时或正在进行时等具有延续性的词汇 在需求分析过程中，尽可能不涉及动作 1.3 状态建模应用场景： 需求建模。 与客户讨论关键事物的变化过程。 进一步细化 用例图 或 用例场景。 OOD/P 软件设计。 关键数据的变化；表达状态机；状态机模式（GOF） 工作步骤： 确定研究对象 这是最难的！系统作为一个对象？所有对象？ 通常是客户关注的业务交易实体，如订单，凭证等 识别状态集合 I， F， S={S0…Sk} 状态不一定是门的开或关闭这样简单属性，可能是属性复杂的组合 事物中包含的 mode 和 state 属性 识别事件和变迁条件 合理性、完整性检查与逻辑分析 终点的可达性 悬挂状态 循环分析（死循环条件与风险） 路径分析（最短路径、关键路径、平均距离） 2. 实例 建模工具： UMLet 14.1.1 stand-alone 2.1 使用 UML State Model 建模对象： 参考 Asg_RH 文档， 对 Reservation/Order 对象建模。 建模要求： 参考练习不能提供足够信息帮助你对订单对象建模，请参考现在 定旅馆 的旅游网站，尽可能分析围绕订单发生的各种情况，直到订单通过销售事件（柜台销售）结束订单。 2.2 研究淘宝退货流程活动图，对退货业务对象状态建模]]></content>
      <categories>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>状态建模</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Session和Cookie]]></title>
    <url>%2F2018%2F05%2F01%2FSession%E5%92%8CCookie%2F</url>
    <content type="text"><![CDATA[1. Cookie概念： 服务器和客户端直接维持状态的解决方案 HTTP无状态：指的是每次客户端向服务器发送HTTP方法的时候，对于服务器而言都是一次新的操作，之前的操作不会对后面的操作有任何影响。都是把每个请求当作一个新的请求。 打个比方： 无状态：每次去同一家餐厅吃饭，无状态的店员都把你当作新客人，每次都重新推荐菜品 有状态：每次去同一家餐厅吃饭，有状态的店员对你之前来过有印象，就当作老顾客，能熟络的叫出你的名字，甚至都知道你喜欢的哪几道菜 目的：Cookie的出现就是为了简单的保留你的登录状态，而不用每次都重复登录 特点： 状态（历史操作记录，Cookie）数据保留在客户端 分为会话Cookie和持久Cookie 机制： 2. Session Session同样是解决HTTP无状态问题的机制，只是将历史数据保留在了服务器端，由服务器来管理 概念： 服务器和客户端直接维持状态的解决方案，有时也指这种解决方案的存储结构（与Cookie不同在于状态信息管理的位置不同，一个在服务器端，一个在客户端） 特点： SessionId是全局的、唯一的 一个客户对应一个SessionId Session在服务端器存储，管理 Session是有生命周期的，会被产生，摧毁 机制： References : https://github.com/astaxie/build-web-application-with-golang/blob/master/zh/preface.md 3. Express.js配置Redis持久化存储Session会话 References: http://www.cnblogs.com/chyingp/p/express-session.html https://segmentfault.com/a/1190000002630691 http://www.runoob.com/redis/redis-commands.html Redis使用 1234redis-server # 启动Redis服务器redis-cli -h host -p port -a password # 连接本地redis服务config set requirepass password # 设置登录密码 Express.js配置 12345678910111213141516171819202122232425var expressSession = require('express-session'); // 该中间件使得req有session属性var RedisStore = require('connect-redis')(expressSession);var redisConfig=&#123; 'cookie' : &#123; 'maxAge' : 1800000 // 30 * 60 * 1000 ms = 30 mins &#125;, 'sessionStore' : &#123; 'host' : '127.0.0.1', 'port' : '6379', 'pass' : 'password', 'db' : 1, 'ttl' : 1800, // 30 * 60 sec = 30 mins 'logErrors' : true &#125;&#125;// var FileStore = require('session-file-store')(session);app.use(expressSession(&#123; name : 'sid', secret : 'zhidan', // 用来对session id相关的cookie进行签名 resave : true, rolling: true, saveUninitialized : false, // 是否自动保存未初始化的会话，建议false cookie : redisConfig.cookie, // 是否每次都重新保存会话，建议false store : new RedisStore(redisConfig.sessionStore) // Redis存储session（也可以选择其他store，比如FileStore()，本地文本存取，）&#125;)); Session使用 12345// 设置Sessionreq.session.site = &#123;number: '110'&#125;;// 读取Sessionvar number = req.session.number;]]></content>
      <categories>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>Session</tag>
        <tag>Cookie</tag>
        <tag>Express.js</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx]]></title>
    <url>%2F2018%2F05%2F01%2FNginx%2F</url>
    <content type="text"><![CDATA[1. Nginx References from Wiki: Nginx is a web server which can also be used as a reverse proxy, load balancer and HTTP cache. Nginx是一个网页服务器，它可以作为反向代理，负载均衡，和HTTP缓存。在高连接并发的情况下，Nginx是Apache服务器不错的替代品。 2. 安装Nginx 两种安装方法： 通过apt-get或yum安装别人预编译的二进制安装包安装 源码编译安装 方式一：二进制安装包安装（推荐）12345sudo apt-get install nginxnginx -V # 查看版本# 服务器配置路径：/etc/nginx/nginx.conf# 程序路径：/usr/sbin 可能会需要遇到缺少模块的提示，使用apt-get命令安装缺失的模块，看提示，缺什么，补什么 方式二：编译安装（不推荐） References: http://www.nginx.cn/install http://www.runoob.com/linux/nginx-install-setup.html https://blog.csdn.net/u013140542/article/details/36070521 安装编译依赖 1sudo apt-get install build-essential libtool gcc automake autoconf make g++ -y 选定源码目录（这里选择/usr/local/src） 1cd /usr/local/src 安装PCRE库（各个版本PCRE库：https://sourceforge.net/projects/pcre/files/pcre/），这里我用8.35版本 12345678910# option1：二进制安装包sudo apt-get install libpcre3 libpcre3-dev -y# option2：源码编译安装cd /usr/local/srcwget http://downloads.sourceforge.net/project/pcre/pcre/8.35/pcre-8.35.tar.gztar -zxvf pcre-8.35.tar.gzcd pcre-8.35./configuremake &amp;&amp; make installpcre-config --version # 检查PCRE版本 安装zlib库。http://zlib.net/zlib-1.2.8.tar.gz 下载最新的 zlib 源码包，使用下面命令下载编译和安装 zlib包： 12345678# option1：系统自带# option2：源码编译安装cd /usr/local/srcwget http://zlib.net/zlib-1.2.11.tar.gztar -zxvf zlib-1.2.11.tar.gzcd zlib-1.2.11./configuremake &amp;&amp; make install 安装SSL（有些服务器没有OpenSSL库，下载地址https://www.openssl.org/source） 12345678# option1：二进制安装包sudo apt-get openssl libssl-dev libperl-dev -yopenssl version -a# option2：源码安装cd /usr/local/srcwget https://www.openssl.org/source/openssl-1.0.1t.tar.gztar -zxvf openssl-1.0.1t.tar.gz 下载编译安装nginx（下载地址下载地址：http://nginx.org/download，这里选择1.6.2版本） 123456789101112131415161718cd /usr/local/srcwget http://nginx.org/download/nginx-1.6.2.tar.gztar zxvf nginx-1.6.2.tar.gzcd nginx-1.6.2./configure --prefix=/usr/local/webserver/nginx --with-http_stub_status_module --with-http_ssl_module # --with-openssl=openssl# configure其他安装配置：# ./configure --sbin-path=/usr/local/nginx/nginx \# --conf-path=/usr/local/nginx/nginx.conf \# --pid-path=/usr/local/nginx/nginx.pid \# --with-http_ssl_module \# --with-pcre=/usr/local/src/pcre-8.37 \# --with-zlib=/usr/local/src/zlib-1.2.8 \# --with-openssl=/usr/local/src/openssl-1.0.1tmake &amp;&amp; make install 安装结果 12345678910111213141516171819Configuration summary + using PCRE library: /usr/local/src/pcre-8.35 + using OpenSSL library: /usr/local/src/openssl-1.0.1 + md5: using OpenSSL library + sha1: using OpenSSL library + using system zlib library nginx path prefix: "/usr/local/webserver/nginx" nginx binary file: "/usr/local/webserver/nginx/sbin/nginx" nginx configuration prefix: "/usr/local/webserver/nginx/conf" nginx configuration file: "/usr/local/webserver/nginx/conf/nginx.conf" nginx pid file: "/usr/local/webserver/nginx/logs/nginx.pid" nginx error log file: "/usr/local/webserver/nginx/logs/error.log" nginx http access log file: "/usr/local/webserver/nginx/logs/access.log" nginx http client request body temporary files: "client_body_temp" nginx http proxy temporary files: "proxy_temp" nginx http fastcgi temporary files: "fastcgi_temp" nginx http uwsgi temporary files: "uwsgi_temp" nginx http scgi temporary files: "scgi_temp" 3. Nginx使用 References: http://www.nginx.cn/76.html 常用命令 12345678nginx # 开启服务器ps -ef | grep nginx # 查看nginx进程否在运行nginx -s stop # 传递中断信号nginx -s reload # 传递重启信号vi /etc/nginx/nginx.conf # 编辑配置 配置说明（/etc/nginx/nginx.conf） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123#运行用户user nobody;#启动进程,通常设置成和cpu的数量相等worker_processes 1;#全局错误日志及PID文件#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#pid logs/nginx.pid;#工作模式及连接数上限events &#123; #epoll是多路复用IO(I/O Multiplexing)中的一种方式, #仅用于linux2.6以上内核,可以大大提高nginx的性能 use epoll; #单个后台worker process进程的最大并发链接数 worker_connections 1024; # 并发总数是 worker_processes 和 worker_connections 的乘积 # 即 max_clients = worker_processes * worker_connections # 在设置了反向代理的情况下，max_clients = worker_processes * worker_connections / 4 为什么 # 为什么上面反向代理要除以4，应该说是一个经验值 # 根据以上条件，正常情况下的Nginx Server可以应付的最大连接数为：4 * 8000 = 32000 # worker_connections 值的设置跟物理内存大小有关 # 因为并发受IO约束，max_clients的值须小于系统可以打开的最大文件数 # 而系统可以打开的最大文件数和内存大小成正比，一般1GB内存的机器上可以打开的文件数大约是10万左右 # 我们来看看360M内存的VPS可以打开的文件句柄数是多少： # $ cat /proc/sys/fs/file-max # 输出 34336 # 32000 &lt; 34336，即并发连接总数小于系统可以打开的文件句柄总数，这样就在操作系统可以承受的范围之内 # 所以，worker_connections 的值需根据 worker_processes 进程数目和系统可以打开的最大文件总数进行适当地进行设置 # 使得并发总数小于操作系统可以打开的最大文件数目 # 其实质也就是根据主机的物理CPU和内存进行配置 # 当然，理论上的并发总数可能会和实际有所偏差，因为主机还有其他的工作进程需要消耗系统资源。 # ulimit -SHn 65535&#125;http &#123; #设定mime类型,类型由mime.type文件定义 include mime.types; default_type application/octet-stream; #设定日志格式 log_format main '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; access_log logs/access.log main; #sendfile 指令指定 nginx 是否调用 sendfile 函数（zero copy 方式）来输出文件， #对于普通应用，必须设为 on, #如果用来进行下载等应用磁盘IO重负载应用，可设置为 off， #以平衡磁盘与网络I/O处理速度，降低系统的uptime. sendfile on; #tcp_nopush on; #连接超时时间 #keepalive_timeout 0; keepalive_timeout 65; tcp_nodelay on; #开启gzip压缩 gzip on; gzip_disable "MSIE [1-6]."; #设定请求缓冲 client_header_buffer_size 128k; large_client_header_buffers 4 128k; #设定虚拟主机配置 server &#123; #侦听80端口 listen 80; #定义使用 www.nginx.cn访问 server_name www.nginx.cn; #定义服务器的默认网站根目录位置 root html; #设定本虚拟主机的访问日志 access_log logs/nginx.access.log main; #默认请求 location / &#123; #定义首页索引文件的名称 index index.php index.html index.htm; &#125; # 定义错误提示页面 error_page 500 502 503 504 /50x.html; location = /50x.html &#123; &#125; #静态文件，nginx自己处理 location ~ ^/(images|javascript|js|css|flash|media|static)/ &#123; #过期30天，静态文件不怎么更新，过期可以设大一点， #如果频繁更新，则可以设置得小一点。 expires 30d; &#125; #PHP 脚本请求全部转发到 FastCGI处理. 使用FastCGI默认配置. location ~ .php$ &#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125; #禁止访问 .htxxx 文件 location ~ /.ht &#123; deny all; &#125; &#125;&#125; 4. Nginx配置HTTPS支持 References: https://segmentfault.com/a/1190000002866627 https://www.jianshu.com/p/9523d888cf77 HTTP和HTTPS HTTP是明文传输，为了增强可靠性，提出了SSL加密协议，进而变成HTTPS（Hyper Text Transfer Protocol over Secure Socket Layer），即HTTP下加入SSL层，HTTPS的安全基础是SSL，因此加密的详细内容就需要SSL。SSL依靠证书来验证服务器的身份，并为浏览器和服务器之间的通信加密。TLS相当于SSL的继承者。 SSL和TLS概念 导入证书 由HTTPS的概念可知：Nginx实现HTTPS需要到CA获取证书，然后在Nginx中配置SSL，添加证书和密钥。 （如果你是找一个知名的ssl证书颁发机构如VeriSign、Wosign、StartSSL签发的证书，浏览器已经内置并信任了这些根证书，如果你是自建C或获得二级CA授权，都需要将CA证书添加到浏览器，这样在访问站点时才不会显示不安全连接。） 我发现腾讯云域名可以免费申请SSL证书，所以就直接用腾讯云域名申请了SSL证书👇（首先要有个腾讯云账号） 1. 选择域名免费版 2. 填写域名申请邮箱 当证书申请通过之后，就会收到邮件和短信提醒，则可以到腾讯云证书管理主页下载证书，然后上传到服务器。 Nginx配置SSL认证 先检查一下Nginx是否支持SSL（第三行 TLS…enabled 就说明支持） 1234$ nginx -Vnginx version: nginx/1.10.3built with OpenSSL 1.1.0f 25 May 2017TLS SNI support enabled Nginx配置SSL有三种方式： 全站SSL，指整个网站的所有页面都通过https访问 部分页面SSL，指部分页面通过https访问 双向SSL认证，上面的方法是用于认证服务端身份的，而没有认证客户端的身份，双向SSL认证则能够做到客户端身份的认证 本文借鉴全站SSL的方式 摘自：https://segmentfault.com/a/1190000002866627 全站做ssl是最常见的一个使用场景，默认端口443，而且一般是单向认证。 下面是通过修改Nginx的conf文件，实现SSL认证配置 12345678910111213141516server &#123; listen 443; server_name example.com; root /apps/www; index index.html index.htm; ssl on; ssl_certificate ../SSL/ittest.pem; ssl_certificate_key ../SSL/ittest.key;# ssl_protocols SSLv3 TLSv1 TLSv1.1 TLSv1.2;# ssl_ciphers ALL:!ADH:!EXPORT56:RC4+RSA:+HIGH:+MEDIUM:+LOW:+SSLv2:+EXP;# ssl_prefer_server_ciphers on;&#125; 如果想把http的请求强制转到https的话： 12345678server &#123; listen 80; server_name example.me; rewrite ^ https://$server_name$request_uri? permanent;### 使用return的效率会更高 # return 301 https://$server_name$request_uri;&#125; ssl_certificate证书其实是个公钥，它会被发送到连接服务器的每个客户端，ssl_certificate_key私钥是用来解密的，所以它的权限要得到保护但nginx的主进程能够读取。当然私钥和证书可以放在一个证书文件中，这种方式也只有公钥证书才发送到client。 ssl_protocols指令用于启动特定的加密协议，nginx在1.1.13和1.0.12版本后默认是ssl_protocols SSLv3 TLSv1 TLSv1.1 TLSv1.2，TLSv1.1与TLSv1.2要确保OpenSSL &gt;= 1.0.1 ，SSLv3 现在还有很多地方在用但有不少被攻击的漏洞。 ssl_ciphers选择加密套件，不同的浏览器所支持的套件（和顺序）可能会不同。这里指定的是OpenSSL库能够识别的写法，你可以通过 openssl -v cipher &#39;RC4:HIGH:!aNULL:!MD5&#39;（后面是你所指定的套件加密算法） 来看所支持算法。 ssl_prefer_server_ciphers on设置协商加密算法时，优先使用我们服务端的加密套件，而不是客户端浏览器的加密套件。 https优化参数 ssl_session_cache shared:SSL:10m; : 设置ssl/tls会话缓存的类型和大小。如果设置了这个参数一般是shared，buildin可能会参数内存碎片，默认是none，和off差不多，停用缓存。如shared:SSL:10m表示我所有的nginx工作进程共享ssl会话缓存，官网介绍说1M可以存放约4000个sessions。 详细参考serverfault上的问答ssl_session_cache。 ssl_session_timeout ： 客户端可以重用会话缓存中ssl参数的过期时间，内网系统默认5分钟太短了，可以设成30m即30分钟甚至4h。 设置较长的keepalive_timeout也可以减少请求ssl会话协商的开销，但同时得考虑线程的并发数了。 提示：在生成证书请求csr文件时，如果输入了密码，nginx每次启动时都会提示输入这个密码，可以使用私钥来生成解密后的key来代替，效果是一样的，达到免密码重启的效果： 1openssl rsa -in ittest.key -out ittest_unsecure.key 可能出现的错误：403 Forbidden 我出现了这个问题，原因是自己的server里面的root设置成’/‘，没有权限访问，所以就被禁了。]]></content>
      <categories>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>HTTPS</tag>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[领域建模与数据库建模实践]]></title>
    <url>%2F2018%2F04%2F28%2F%E9%A2%86%E5%9F%9F%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BB%BA%E6%A8%A1%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[1. 领域建模 References from Wiki: A domain model is a system of abstractions that describes selected aspects of a sphere of knowledge, influence or activity (a domain[3]). The model can then be used to solve problems related to that domain. The domain model is a representation of meaningful real-world concepts pertinent to the domain that need to be modelled in software. The concepts include the data involved in the business and rules the business uses in relation to that data. A domain model generally uses the vocabulary of the domain, thus allowing a representation of the model to be communicated to non-technical stakeholders. 领域模型是描述知识领域，影响领域或活动领域（领域）的抽象系统。该模型可以用来解决与该域相关的问题。领域模型是与需要在软件中建模的领域相关的有意义的现实世界概念的表示。这些概念包括涉及业务的数据和业务使用的与该数据相关的规则。领域模型通常使用领域的词汇表，从而允许将模型的表示传达给非技术利益相关者。 a. 阅读 Asg_RH 文档，按用例构建领域模型。 按 Task2 要求，请使用工具 UMLet，截图格式务必是 png 并控制尺寸 说明：请不要受 PCMEF 层次结构影响。你需要识别实体（E）和 中介实体（M，也称状态实体） 在单页面应用（如 vue）中，E 一般与数据库构建有关， M 一般与 store 模式 有关 在 java web 应用中，E 一般与数据库构建有关， M 一般与 session 有关 Asg_RH 用例图回顾 UMLet下载 UMLet usage: cd UMLet &amp;&amp; bash umlet.sh # Linux and Unix bash shell command 用例 领域模型 Search hotel by city Make reservation with hotel and room type Pay for basket with reservation 2. 数据库建模 References from Wiki: A database model is a type of data model that determines the logical structure of a database and fundamentally determines in which manner data can be stored, organized and manipulated. The most popular example of a database model is the relational model, which uses a table-based format. 数据库模型是一种数据模型，它决定了数据库的逻辑结构，从根本上决定了数据可以存储，组织和操作的方式。数据库模型最流行的例子是关系模型，它使用基于表格的格式。 b. 数据库建模(E-R 模型) 按 Task 3 要求，给出系统的 E-R 模型（数据逻辑模型） 建模工具 PowerDesigner（简称PD） 或开源工具 OpenSystemArchitect 不负责的链接 http://www.cnblogs.com/mcgrady/archive/2013/05/25/3098588.html 导出 Mysql 物理数据库的脚本 简单叙说 数据库逻辑模型 与 领域模型 的异同 下面是用Mysql Workbench绘制的ER图 Mysql Workbench导出的脚本：Asg_Hotel.sql 比较数据逻辑模型和领域建模异同 同 异 数据逻辑模型和领域模型 1. 以识别主体为绘制的开端，以填充属性为结束2. 主体对象之间都是用一条直线表示关联关系 1. 数据逻辑模型包含更多接近底层数据类型的内容，领域模型则更加语义化一些，没有定义数据类型2. 数据逻辑模型对一对多、多对多的情况的描绘可能需要中间实体，领域模型可以直接表示一对多的情况]]></content>
      <categories>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>领域建模</tag>
        <tag>数据库建模</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Halstead 复杂度]]></title>
    <url>%2F2018%2F04%2F22%2FHalstead%2F</url>
    <content type="text"><![CDATA[1. Halstead1.1 概念Halstead complexity measures（霍尔斯特德软件复杂度度量方法）是一种软件度量方法。 霍尔斯特德：软件复杂度度量，应该要反映不同的程序语言中算法的实现方式，又要独立于使用的平台语言。这些度量可以由静态代码中的计算而得。 1.2 计算Halstead根据语句行的操作符和操作数的数量计算程序复杂度 操作符和操作数的数量越大，程序结构越复杂 操作符包括语言保留字、程序调用、数学运算符、以及有关分隔符 操作数可以是常数和变量 Halstead复杂度度量 设$n_1$表示程序中不同的操作符个数，$n_2$表示不同操作数的个数，$N_1$表示程序中出现的操作符的总数，$N_2$表示程序中出现操作数的总数 Halstead 程序词汇表长度 Program vacabulary: $n = n_1 + n_2$ 实际Halstead长度 Program length: $N = N_1 + N_2$ 以N^表示程序的预测长度 Calculated program length: N^ $= n_1 log_2(n_1) + n_2 log_2(n_2) $ Halstead的重要结论之一是：程序的时间长度N和预测长度N^非常接近，这表明即时程序还未编写完y呢预先估算出程序的实际长度N 其他计算公式 程序的体积，容量 Volume: $V = (N) log_2(n)$ ，表示程序的词汇复杂度 程序级别 Level: $\text {L^} = (2/n_1) \times (n_2/N_2)$ ，表示程序最紧凑形式的程序量和实际程序量的比，反映程序的效率 程序难度 Difficult: $D = 1 / \text {L^}$ ， 表示程序算法的困难程度 编程工作量 Effort: $ E = V \times D = V / \text {L^} $ 智能级别 $I = \text {L^} \times E$ 语言级别 $ L’ = \text {L^} \times \text {L^} \times V$ 编程时间(hours) $\text{T^} = E / (S\times f), \ \ \ S = 60\times 60, f = 18$ 平均语句大小 $N/语句数$ 程序错误预测值 $ B = N \times log_2 (n_1 + n_2)/3000$ 缺点 仅考虑程序数据量和程序体积，不考虑程序控制流的情况。 不能从根本上反映程序复杂性。 2. Example 计算下列程序的 Halstead 复杂度的10项内容 12345678910#include &lt;stdio.h&gt;#include &lt;math.h&gt;int main() &#123; float a, b, c, mean; scanf("%f %f %f", &amp;a, &amp;b, &amp;c); mean = a * b * c; mean = pow(mean, 1.0 / 3.0); printf("Geometric Mean = %f", mean); return 0;&#125; 指标 数值 项 不同的操作符个数 $n_1$ 18 #,include, &lt;&gt;, stdio.h, math.h, int, main, (), {}, float, scanf, &amp;, =, *, pow, /, printf, return 不同的操作数个数 $n_2$ 9 a, b, c, mean, &quot;%f %f %f&quot;, 1.0, 3.0, &quot;Geometric Mean = %f&quot;, 0 操作符个数 $N_1$ 28 - 操作数个数 $N_2$ 19 - 程序词汇表长度 $n = n_1 + n_2$ 27 - 简单长度 $N = N_1 + N_2$ 47 - 程序预测长度 $\text{N^} = n_1log(n_1) + n_2log(n_2)$ 106.389629896 - 程序的体积，容量 Volume: $V = (N) log_2(n)$ 67.2740969155 - 程序级别 Level: $\text {L^} = (2/n_1) \times (n_2/N_2)$ 0.05263157894 - 程序难度 Difficult: $D = 1 / \text {L^}$ 19 - 编程工作量 Effort: $ E = V \times D = V / \text {L^} $ 1278.13 - 智能级别 $I = \text {L^} \times E$ 67.2740969155 - 语言级别 $ L’ = \text {L^} \times \text {L^} \times V$ 0.18181696 - 编程时间(hours) $\text{T^} = E / (S\times f), \ \ \ S = 60\times 60, f = 18$ 0.01972222222 - 平均语句大小=$N/语句数$ 5.222222222 - 程序错误预测值 $ B = N \times log_2 (n_1 + n_2)/3000$ 0.07449323753 -]]></content>
      <categories>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>Halstead</tag>
        <tag>软件测试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jupyter notebook远程连接]]></title>
    <url>%2F2018%2F04%2F18%2Fjupyter-notebook%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[1. 前言在服务器上跑代码是件略微复杂的事情，一般来说有下面两种做法： 在本地调试完代码后，把本地的代码通过持续集成（CI）或者scp命令或者git的方式远程部署到服务器上。 直接在远程终端的vim等命令行编辑器编辑远程终端代码，这对于不熟悉vim操作的人来说是不友好的，而且还需要预先安装vim的各种插件以方便编码。 最近在集群里面跑python的代码的我就想到，能不能把我熟悉的jupyter notebook通过ssh结合起来，转发向远程终端jupyter notebook端口访问的http请求，使得在本地的notebook轻松调试远程终端的代码。 2. 基础2.1 jupyter notebook The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more. Jupyter Notebook是一款开放源代码的Web应用程序，允许您创建和共享包含实时代码，方程式，可视化和叙述文本的文档。用途包括：数据清理和转换，数值模拟，统计建模，数据可视化，机器学习等等。 安装jupyter notebook 这里不做赘述：conda环境安装 运行：jupyter notebook 则会开启一个服务器监听本地的一个端口，同时会产生一个token。如果浏览器访问这个端口则需要输入当前token，则可以进入notebook的界面。 如： 1jupyter notebook --no-browser --port=8890 # 指定无须自动打开浏览器；指定端口8890 2.2 bash重定向命令 预先讲解后面可能用到的bash重定向内容 一般情况下，每个 Unix/Linux 命令运行时都会打开三个文件： 标准输入文件(stdin)：stdin的文件描述符为0，Unix程序默认从stdin读取数据。 标准输出文件(stdout)：stdout 的文件描述符为1，Unix程序默认向stdout输出数据。 标准错误文件(stderr)：stderr的文件描述符为2，Unix程序会向stderr流中写入错误信息。 默认情况下，command &gt; file 将 stdout 重定向到 file，command &lt; file 将stdin 重定向到 file。 References: http://www.runoob.com/linux/linux-shell-io-redirections.html 12345command1 &gt; file1 # 将command1产生的打印结果覆盖输出到file1中command1 &gt;&gt; file1 # 将command1产生的打印结果连接输出到file1中command1 &lt; file1 # 将file1重定向到command1中，如 cat &lt; file1n &gt;&amp; m # 将输出文件n和m合并# 需要注意的是文件描述符 0 通常是标准输入（STDIN），1 是标准输出（STDOUT），2 是标准错误输出（STDERR）。如果希望将 stdout 和 stderr 合并后重定向到 file，可以这样写command &gt; file 2&gt;&amp;1 2.3 nohup命令 nohup就是不挂起的意思( not hang up)，该命令可以在你退出帐户/关闭终端之后，使进程忽略hang up信号，继续运行 123nohup command # 将输出重定向到nohup.out文件nohup command &amp; # 加入&amp;放入后台运行nohup command &gt;filename 2&gt;&amp;1 &amp; # 标准输出和错误输出重定位到文件。 2.4 SSH SSH 是一种加密协议，用于网络主机之间的远程登录 SSH协议下的口令登录过程：远程主机将自己的公钥发给用户，用户用该公钥加密登录口令，发送给远程主机，远程主机用自己的密钥解密，验证用户登录口令是否正确。（这过程中的信息及时是明文的，被截获了也无法破解） 还有一种是公钥登录：用户本地生成公钥和私钥（生成私钥时可以设置本地口令），然后把公钥给远程主机；当进行用户登录时，远程主机将随机字符发送给用户，用户用私钥加密返回密文给远程主机，如果主机用公钥可以解密得到原来的随机字符，就可以验证用户身份，给予登录权限。 相关命令： 123456789101112ssh-keygen # 生成本地 ~/.ssh 文件夹下的公钥和密钥ssh-copy-id user@host # 将公钥发送给远程主机，用于公钥登录# ubuntu启动ssh服务service ssh restart# debian启动ssh服务/etc/init.d/ssh restartssh user@host 'mkdir -p .ssh &amp;&amp; cat &gt;&gt; .ssh/authorized_keys' &lt; ~/.ssh/id_rsa.pub # 将本地id_rsa.pub文件里的公钥复制到远程终端中的authorized_keys文件末尾ssh -L port1:host2:port2 host3 # host2是相对host3而言，如果host2=127.0.0.1就相当于是host3ssh -N -L 127.0.0.1:8892:127.0.0.1:8892 user@host # -N 连接后不进入远程终端。-L 通过第三个服务器转发两个端口之间的数 3. SSH远程连接jupyter notebook 在远程服务器运行： 1nohup jupyter notebook --no-browser --port=34922 &gt; ~/jupyter.txt 2&gt;&amp;1 &amp; # 后台开启远程服务器的jupyter notebook，并将输出放到jupyter.txt文件里。 这时jupyter notebook的服务端口和token就会在jupyter.txt里面，所以将其拷贝到本地 1scp user@host:~/jupyter.txt ./ &amp;&amp; cat jupyter.txt # scp远程拷贝文件到本地，同时显示jupyter.txt的内容 在那道jupyter.txt的内容之后，就可以直接创建SSH转发连接（这里假如在34922端口开启的notebook） 1ssh -N -L 34922:127.0.0.1:34992 user@host # ssh转发连接 最后就可以在本地浏览器里粘贴jupyter.txt里面的地址：http://localhost:34992?token=333402uhdixx，在浏览器进入远程终端的notebook。 总结 尽管过程略微繁琐，但是学到的一些基础知识还是很多的，这里我一般都是写到.sh脚本里面直接运行，所以可能会稍微简化了这个过程 现在在尝试用更简单的步骤访问，所以有改进肯定会在此更新，敬请期待。 4. （续）jupyter notebook远程访问之前的ssh连接的方法绕了一个大弯，原因是jupyter notebook启动时候的配置指定本地访问，但是其实可以指定任意ip访问，所以可以直接在外部访问。其次通过修改配置文件，还能修改密码，具体教程就不写了。如下👇 给jupyter设置密码以能远程访问的方法 References: https://www.ibm.com/developerworks/cn/linux/l-cn-sshforward/ http://www.ruanyifeng.com/blog/2011/12/ssh_remote_login.html http://www.ruanyifeng.com/blog/2011/12/ssh_port_forwarding.html jupyter ssh远程转发实战：https://blog.csdn.net/yijuan_hw/article/details/68945694]]></content>
      <categories>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>SSH</tag>
        <tag>jupyter notebook</tag>
        <tag>nohup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CUDA和cuDNN]]></title>
    <url>%2F2018%2F04%2F18%2FCUDA%2F</url>
    <content type="text"><![CDATA[1. CUDA，cuDNN概念 NVIDIA官网CUDA: http://www.nvidia.cn/object/cudazone-cn.html NVIDIA官网cuDNN: https://developer.nvidia.com/cuDNN NVIDIA官网深度学习SDK: https://developer.nvidia.com/deep-learning-software 1.1 深度学习SDK The NVIDIA Deep Learning SDK provides powerful tools and libraries for designing and deploying GPU-accelerated deep learning applications. It includes libraries for deep learning primitives, inference, video analytics, linear algebra, sparse matrices, and multi-GPU communications. NVIDIA的深度学习SDK提供一些有力的工具和库用于GPU加速深度学习应用，包括一些库关于深度学习、推导、视频分享、线性代数、稀疏矩阵、和多GPU通信 1.2 cuDNN Deep Learning Primitives (cuDNN): High-performance building blocks for deep neural network applications including convolutions, activation functions, and tensor transformations 一个高性能库为深度神经网络应用包括卷积、激活函数、张量转换等 1.3 CUDA The Deep Learning SDK requires CUDA Toolkit, which offers a comprehensive development environment for building new GPU-accelerated deep learning algorithms, and dramatically increasing the performance of existing applications 深度学习SDK需要CUDA工具包，它提供了一个全面的开发环境，用于构建新的GPU加速深度学习算法，并显着提高现有应用程序的性能。深度学习SDK需要CUDA工具包，它提供了一个用于构建新GPU的全面开发环境加速的深度学习算法，并显着提高现有应用程序的性能 1.4 题外话 SDK : 软件开发工具包（SoftwareDevelopmentKit,SDK）一般是一些被软件工程师用于为特定的软件包、软件框架、硬件平台、作业系统等创建应用软件的开发工具的集合。 API：应用程序接口（英语：ApplicationProgrammingInterface，简称：API），又称为应用编程接口，就是软件系统不同组成部分衔接的约定。 Library:用于开发软件的子程序集合。库和可执行文件的区别是，库不是独立程序，他们是向其他程序提供服务的代码。 Framework:通常指的是为了实现某个业界标准或完成特定基本任务的软件组件规范，也指为了实现某个软件组件规范时，提供规范所要求之基础功能的软件产品。 toolkit (plural toolkits) An assembly of tools. (computing) A set of basic components for developing software. 1.5 CUDA vs cuDNN CUDA（Compute Unified Device Architecture），是显卡厂商NVIDIA推出的运算平台。 CUDA™是一种由NVIDIA推出的通用并行计算架构，该架构使GPU能够解决复杂的计算问题。 cuDNN是专门针对Deep Learning框架设计的一套GPU计算加速方案，目前支持的DL库包括Caffe，ConvNet, Torch7等。 两者关系：cuda针对并行计算加速，cudnn针对神经网络的计算加速，cudnn需要在有cuda的基础上进行。 2. 安装 这里不做安装教程了，网上具体都有 官网CUDA安装教程：https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html 官网cuDNN安装教程：https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html 1nvcc -V # 集群cuda驱动检查安装 3. 使用http://pytorch.org/docs/master/notes/cuda.html]]></content>
      <categories>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>CUDA</tag>
        <tag>cuDNN</tag>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Anaconda入门知识]]></title>
    <url>%2F2018%2F04%2F18%2FAnaconda%2F</url>
    <content type="text"><![CDATA[1. Anaconda和conda概念conda是包管理器和环境管理器，相当于结合了pip和virtualenv的功能 Anaconda在英文中是“蟒蛇”，Anaconda是一个包含180+的科学包及其依赖项的发行版本。 这里先解释下conda、anaconda这些概念的差别。conda可以理解为一个工具，也是一个可执行命令，其核心功能是包管理与环境管理。包管理与pip的使用类似，环境管理则允许用户方便地安装不同版本的python并可以快速切换。Anaconda则是一个打包的集合，里面预装好了conda、某个版本的python、众多packages、科学计算工具等等，所以也称为Python的一种发行版。其实还有Miniconda，顾名思义，它只包含最基本的内容——python与conda，以及相关的必须依赖项，对于空间要求严格的用户，Miniconda是一种选择。作者：PeterYuan链接：https://www.jianshu.com/p/2f3be7781451來源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 Anaconda 附带了一大批常用数据科学包，它附带了 conda、Python 和 150 多个科学包及其依赖项。因此你可以立即开始处理数据。 Anaconda 是在 conda（一个包管理器和环境管理器）上发展出来的。 Anaconda安装镜像：https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/ 2. conda使用2.1 管理安装包在数据分析中，你会用到很多第三方的包，而conda（包管理器）可以很好的帮助你在计算机上安装和管理这些包，包括安装、卸载和更新包。 12345678conda listconda upgrade --allconda search nums# 管理包操作：安装 卸载 更新conda install package_nameconda remove package_nameconda update package_name conda 还会自动为你安装依赖项。例如，scipy 依赖于 numpy，因为它使用并需要 numpy。如果你只安装 scipy (conda install scipy)，则 conda 还会安装 numpy（如果尚未安装的话）。 2.2 管理环境conda 可以为你不同的项目建立不同的运行环境。 安装nb_conda用于notebook自动关联nb_conda的环境。 创建环境 12conda create -n env_name package_namesconda create -n py3 pandas # 要创建环境名称为 py3 的环境并在其中安装 numpy 创建环境时，可以指定要安装在环境中的 Python 版本 123conda create -n py3 python=3 conda create -n py2 python=2conda create -n py python=3.6 # 如果你要安装特定版本（例如 Python 3.6） 进入环境 1source activate my_env 离开环境 1source deactivate 共享环境 12conda env export &gt; environment.yaml # 将你当前的环境保存到文件中包保存为YAML文件（包括Pyhton版本和所有包的名称）。conda env update -f=/path/to/environment.yml # 其中-f表示你要导出文件在本地的路径，所以/path/to/environment.yml要换成你本地的实际路径 1pip install -r /path/requirements.txt # 对于不使用conda 列出环境 1conda env list 删除环境 1conda env remove -n env_name References: https://www.zhihu.com/question/58033789 https://www.jianshu.com/p/2f3be7781451 anaconda官方文档：https://www.anaconda.com conda官方文档：https://conda.io/docs/user-guide/tasks/index.html 安装Anaconda常见问题：https://zhuanlan.zhihu.com/p/34337889 2.3 conda源科大： 12345678conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/main/conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/menpo/conda config --set show_channel_urls yes]]></content>
      <categories>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>Anaconda</tag>
        <tag>conda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从逻辑回归的角度看SVM]]></title>
    <url>%2F2018%2F04%2F16%2Flogist2SVM%2F</url>
    <content type="text"><![CDATA[SVM是一种线性分类方法，数据量少的时候效果相对较好 线性模型选择经验：对于数据量大的线性方法，其实效果是差不多的，区别在于是否算法能够并行化，是否可以做到实时处理 1 逻辑回归回顾1.1 logistic数学模型逻辑回归的模型是S型曲线模型（sigmoid），所以一般使用的模型函数是$h(\theta^Tx) = \frac 1 {1+e^{\theta^Tx}}$ 有时可以对$\theta^Tx$前面加个参数调整S形状。 为什么叫logistic？ 摘自统计学习方法： 1.2 损失函数为了衡量模型的好坏我们需要提出一个准则，即loss模型对样本预测结果的损失函数 对某个样本$(x_i, y_i)$预测结果的效果判断 : L(\theta) = \begin{cases} - log(h_{\theta}(x_i)) & \quad \text{if } y_i = 1 \\ -log(1- h_{\theta}(x_i)) & \quad \text{if } y_i = 0 \end{cases}合并形式： L(\theta) = - (y_i log(h_{\theta}(x_i)) + (1-y_i) log(1-h_{\theta}(x_i))) 损失函数为什么是取对数呢？ 从图形方面考虑是为了使得产生下面两种loss形状 符合最大熵模型中的一种极大似然估计的方法👇 L(\theta) = \prod h(\theta^Tx_i)^{y_i} h(\theta^Tx_i)^{1-y_i} \\ -log(L(\theta)) = -(y_0*log(h(\theta^Tx_0)) + (1-y_0)*log(h(\theta^Tx_0))+….)2. SVM2.1 SVM的损失函数回顾logistic回归的损失函数的图像如下（这里应该是曲线的） 当在SVM的情况下的loss是： 可以看到一个是loss的输入不同，一个是$h(\theta^Tx)​$，一个是$\theta^Tx​$；另一个是$cost_0, cost_1​$的不同，logistic回归的loss是log函数的平滑曲线，SVM的loss函数是直的折线 折线表达式如何表示？$cost_1(z) = max{0, \text {xxx}}$ 对loss形状的修改可以换成类似的其他函数，这样就可以开发成别的算法 2.2 SVM模型 SVM模型Loss表达式中各个式子的含义，推导出SVM模型的几何含义 $min \frac 1 2 \theta^2$ 这是$\theta$的模的平方 模型目标 \begin{cases}\theta^Tx^{(i)} \ge 1 & \quad \text {if } y^{(i)} = 1 \\ \theta^Tx^{(i)} \le -1 & \quad \text {if } y^{(i)} = 0 \end{cases} 这里的$\theta^Tx^{(i)}$ 可以看做是$x$向量在$\theta$向量上面的投影与$\theta$的模的乘积，而这里的$\theta$就当作支持向量机里面超平面的法向量（超平面就是将样本安照标签分类的平面） 模型解释：模型是一个超平面$\theta$ ，对于在这个超平面两侧的平面$\theta^Tx = 1, \ \theta^Tx = -1$，将数据分类到这两个平面两侧，所以要满足: \begin{cases}\theta^Tx^{(i)} \ge 1 & \quad \text {if } y^{(i)} = 1 \\ \theta^Tx^{(i)} \le -1 & \quad \text {if } y^{(i)} = 0 \end{cases} 而要求两个平面之间的距离尽量大，这两个平面之间的距离（margin）就是$\frac 2 {|\theta|}$ ，所以最大化$\frac 2 {|\theta|} $，可以转换为最小化$\frac 1 2 \theta^2$ ，即$min \frac 1 2 \theta^2$ 通过上面的分析，我们可以看到SVM的几何含义就是：找出一个超平面使得将样本点按照标签分类，同时使得离超平面最近的样本点的距离最远（margin足够大）。 2.3 Insparable Data - Hinge Loss 对于部分线性不可分的样本点，如何处理？ ：容许一定的错误，加入错误惩罚机制，使得尽量分开数据点]]></content>
      <categories>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>Logistic Regression</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图像检测的神经网络模型发展总结]]></title>
    <url>%2F2018%2F04%2F14%2F%E5%9B%BE%E5%83%8F%E6%A3%80%E6%B5%8B%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[R-CNN的提出是开创性的，它是第一次结合了神经网络的算法在图像检测中得到应用 这里我打算将我所理解的图像检测的神经网络发展脉络做个总结，顺便理清一下我的头绪 1. 图像检测与图像识别问题不同，图像检测问题需要将目标在原图像中的位置识别出来，再做对目标的图像识别。 可以说是加入了目标位置检测的图像识别 2. R-CNN Regions with CNN features 2.1 基本架构图 2.2 基本结构分成三个模块 Region Proposal 概念：从原图像中提取目标可能的存在的候选区域 在这篇文章中使用的是传统的Selective Searching算法（具体我也没有深究） 图像特征提取 通过一个多层卷积网络提取图像的特征，为下个模块提供图像的特征输入 SVM分类器 从上个模块中得到的图像特征再做图像的分类，通过支持向量机的算法做分类学习 2.3 总结 Selective Searching的方法是传统的方法，其中只能在CPU中使用，计算复杂度高 对于在最后使用SVM做分类器我不是很懂，本来可以直接在网络中训练分类，可是却没有这样 3. SPPnet SPPnet，全称Sptial Pyramid Pooling Net；下面摘自：http://www.cnblogs.com/rocbomb/p/4428946.html SPP的思想来源于SPM，然后SPM的思想来源自BoW。 关于BoW和SPM，找到了两篇相关的博文，就不在这里展开了。 第九章三续：SIFT算法的应用—目标识别之Bag-of-words模型 Spatial Pyramid 小结 R-CNN待解决问题：R-CNN在Region Proposal中得到的那些Proposals区域大小规格不同，需要在warp或者是crop操作，对于图像的信息有一点影响，导致图像的信息缺失，或者变异 SPP层：为了解决上面的问题SPP提出了SPP池化层，通过对conv5得到的feature map做统一规格的池化操作，最终可以得到大小一样的输出（主要是因为卷积层对统一大小没有要求，只有在全连接层才会有要求，所以需要在卷积层之后做统一规格的pooling）。 金字塔：SPP之所以叫金字塔，是因为对conv5的feature map做的是多个尺度的规格化pooling（如下图），任意的输入得到的都是：4x4, 2x2, 1x1大小的feature map。 SPP后续操作都是和R-CNN一致 问题： 无法同时调参tuning卷积层和全连接层 需要做SVM，之后额外做bounding box Regressor 4. Fast R-CNN4.1 基本架构图 4.2 基本模块 Region Proposal 第一部分基本还是用老生常谈的ss，除去考虑这个模块，Fast R-CNN基本实现了端到端的思想（end-to-end） CNN分类器+Bounding Box Regressor conv feature map：将整个图像输入多个卷积层+池化层得到卷积层输出的特征图 fixed-length feature：求到原图的proposals区域在conv feature map中的映射位置，输入到RoI pooling layer，使得输出固定大小 RoI feature vector：将上面固定大小的特征输入多个全连接层，得到特征向量 sibling output layers： outputs layers：输出对应区域在各个类别的检测概率 bbox regressor：输出对应区域学习优化的框的大小（用于调整bounding box的尺寸和位置） Multi-task loss output layer的loss bbox regressor的loss 4.3 总结 改进R-CNN和SPPnet中的缺陷 R-CNN和SPPnet的多阶段处理问题 时间空间开销大的问题 图像检测太慢 取消了SVM分类器，用网络代替 贡献： 训练是单阶段的 提出了Bounding Box Regressor提高Region Proposal的精度 5. Faster R-CNN Fast R-CNN中仍有的问题：Region Proposal阶段仍旧用的是传统方法，使用的是CPU，计算速度慢 5.1 RPN Region Proprosol Network： Faster R-CNN提出了RPN网络的概念为的是将Regon Proposal融入网络，使得真正做到端到端的检测模型 5.1.1 基本架构图 5.1.2 机制 RPN所需要解决的就是提出有物体的区域的大致位置，位置的矫正可以在后面通过Bounding Box Regressor实现 目标：就是输入一张图片，输出图片中含有物体的区域位置（x，y，w，h） ​ RPN层是全连接网络，输入的获取是通过一个nxn的sliding window在conv feature map上取值（conv feature map是通过多层卷积层的结果，如：VGG，ZFNet），feature map的像素点要作为sliding window的中心做一次RPN操作。 ​ sliding window中的值通过一个全连接层得到一维向量，一维向量再在两个并行的全连接网络（sibling layer），一个是cls layer，另一个是reg layer； ​ 这个sliding window的中心可以映射回原图的区域，得到原图的对应区域（anchor）的中心点，而我们以这个中心为固定点，设定几个固定尺度，固定缩放的区域，这样一个conv feature map的点可以映射回去多个区域（anchor）；这里我们假定是k个anchors，那对于cls layer 就有2k个输出，代表（有物体/没有物体）的得分，reg layer就有4k个输出（代表有物体的区域的对应位置）。 anchor 👇 ​ 训练好RPN后就能，将提取的proposals用到Fast R-CNN中，进行分类和bbox regressor；其实这里Fast R-CNN需要和RPN网络是共享特征的，即输入的特征来自一个卷积网络的特征提取；那么这样子多个loss如何训练呢，这里需要一定的训练技巧：轮流训练，联合训练（由于我没有操作经验，这里没有细说） References: 论文笔记 《Fast R-CNN》 【目标检测】Faster RCNN算法详解 rcnn spp_net 你应该知道的9篇深度学习论文（CNNs 理解） 分类以及目标检测发展脉络——从12到17]]></content>
      <categories>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>Faster R-CNN</tag>
        <tag>深度学习</tag>
        <tag>R-CNN</tag>
        <tag>SPPnet</tag>
        <tag>Fast R-CNN</tag>
        <tag>图像检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hough Transform]]></title>
    <url>%2F2018%2F04%2F12%2FHough-Transform%2F</url>
    <content type="text"><![CDATA[1. 基础知识1.1 curve fitting 和 interpolation 拟合 用一个数学模型拟合离散数据的过程 目标是为了获得一个数学模型与离散数据点之间的平均误差最小化 所有数据点不一定刚好在这个数学模型上 插值 绘制出一条光滑的曲线，保证所有数据点都在这条曲线上 1.2 最小二乘法最小二乘法是用于近似求解方程组的线性代数方法 曲线拟合最小化平均误差 等价于导函数为零 等价于求解方程组 使用最小二乘求最小二乘解： Ax = b \\ A^TA x = A^T b \\ (A^TA)' A^TAx = (A^TA)' A^T b \\ x = (A^TA)' A^T b1.3 curve fitting运用于图像边缘建模对已有的边缘图中的边缘像素点用直线建模 存在困难 多条直线如何通过fitting拟合？哪些点属于要拟合的这个子集（枚举？不可能） 噪声点对线性拟合的影响 缺失点问题 解决问题思想：voting 不可能穷尽所有子集去拟合直线 用特征点去投票，投模型 噪声也可能投票，但是投票是规则的，不影响 2. Hough Transform （霍夫变换）2.1 image space 到 hough space image space：图像空间是指原图像的二维空间（用坐标$(x, y)$ 表示），对于image space中的某一条直线可以用$y = m_0 x + b_0$ 表示。 hough space：霍夫空间，也叫参数空间，是指上述直线中$(m_0, b_0)$构成的空间 映射关系（当hough变换是在直线坐标系下时）： image space(x, y) hough space(m, b) 直线：$y = m_0x+b_0$ 点：$(m_0, b_0)$ 点：$(x_0, y_0)$ 直线：$b = -x_0m+y_0$ hough空间的直线：理解成在image空间里过某一点可能的直线的所有可能$m, b$ 的点构成的直线 hough空间的直线交点：理解成image空间中分别过两个点的所有直线中，重合的情况，即image空间中两点确定一条线 2.2 hough变换可能的存在的问题如何表示image space中竖直的直线？ 解决：hough space用极坐标参数，在image space用极坐标表示一条直线 $极坐标系直线方程：r = acos\theta + b sin\theta，表示过极坐标点(r, \theta) ，且与极坐标点与原点的连线垂直的直线，则(a, b)可以当做hough 空间$ 2.3 hough变换机制 image space是离散的，是一个个单位像素点组成的 hough space同理，可以用一个个cell表示，初始为0 在image space（已经canndy处理），然后对每个边缘像素点hough变换得到对应曲线，在hough space中曲线位置的cell（像素点）的值+1处理。 最后hough space会得到一个灰度图，最亮的cell（像素点）就是一个peak，就是代表一个image space中的直线。 Difficulties hough space的cell多大（分辨率多大？） 最终用多少条直线 通过筛选灰度值高的peak 对筛选后的点做聚类 哪个点属于哪个直线 选择哪些点里直线近，近的就确定直线，并去掉那些确定后的点 选择剩下的点重新做hough]]></content>
      <categories>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>Computer Vision</tag>
        <tag>Hough Transform</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UML绘图学习]]></title>
    <url>%2F2018%2F04%2F11%2FUML%E7%BB%98%E5%9B%BE%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[1. 团队作业 项目Dashboard 软件规格说明书（含用例） 2. 个人作业2.1 用例建模 用例( Use Case )是一种描述产品需求的方法，使用用例的方法来描述产品需求的过程就是用例模型，用例模型是由用例图和每一个用例的详细描述文档所组成的。 用例建模是UML建模的一部分，它也是UML里最基础的部分。用例建模的最主要功能就是用来表达系统的功能性需求或行为。用例图重点描述用户需求。 它描述需求、用户和主要组件之间的关系。 a. 阅读 Asg_RH 文档，绘制用例图。 按 Task1 要求，请使用工具 UMLet，截图格式务必是 png 并控制尺寸 b. 选择你熟悉的定旅馆在线服务系统（或移动 APP），如绘制用例图。并满足以下要求： 对比 Asg_RH 用例图，请用色彩标注出创新用例或子用例 尽可能识别外部系统，并用色彩标注新的外部系统和服务 c. 对比两个时代、不同地区产品的用例图，总结在项目早期，发现创新的思路与方法 相同用例/系统 不同用用例/系统 1. 搜索酒店2. 预订住房3. 支付4. 取消订单5. 输入筛选条件6. 选择房间7. 提交订单8. 确认订单信息 1. 短信系统2. 交通地图系统3. 搜索地标周边酒店4. 查看酒店交通地图5. 收藏酒店6. 线下支付7. 选择套餐 创新思路和方法： 将服务打包化，类似套餐之类的服务集合，减少部分用例，体验更好 考虑实际中客户可能面对的问题，尽可能在一个app中提供解决方法（如：交通路线图） 对于支付方式，应该提供更多的选择 在原有的用例图基础上，添加创新性子用例，添加创新性父用例 d. 请使用 SCRUM 方法，在（任务b）用例图基础上，编制某定旅馆开发的需求 （backlog 产品特性） SCRUM：敏捷开发 backlog：产品backlog由所有的功能特性，包括业务功能，非业务功能（技术、架构和工程实践相关），提升点以及缺陷的修复等组成。这些内容也是将来产品版本发布的主要内容。一个完整的backlog是一个的蓝图，可以根据它来把产品改造成为我们期望的样子。 但是在Scrum中，Backlog是根据产品和产品使用环境的演化而不断演化的。所以Backlog是动态的，我们会持续的改变它去确保我们的产品是最合理的，最有竞争力的，最有价值的。 当我们去看产品的backlog的时候，优先级是一个重要的视角，优先级越高的backlog需要越清晰，越详细。对于优先级低的backlog，详细程度会越低，直到几乎我们不能认为它是一个backlog项（非常低的优先级，只相当于一个占位符，来用做提醒）。 backlog列表包含以下字段 ID 是统一的标识符，自增的数字 名称 是简短的、描述性的需求(故事、特性)名；它必须要含义明确，这样开发人员和产品负责人才能大致明白是什么东西，以跟其他需求(故事、特性)区分开。 估算 对每个backlog项做估算（包括成本，复杂度，风险，功能点）。优先级越高的Backlog估算要越精确，在估算的过程中可能会导致backlog的优先顺序有可能随之发生变化（对于那些很重要，并且可以快速解决的问题可以先做）。 我们要经常做估算。 优先顺序（重要性）每个backlog项都有优先级，这些backlog项按照优先次序排行队列放在backlog列表中。在评估的过程中 如何演示 指大致描述一下在验收时，如何才能进行演示，本质就是一个简单的测试规范 注释 指相关信息、解释说明和对其它资料的引用等等。一般都非常简短或没有。 ID Name Est Imp How to do demo Note 1 创建住宿订单 100 10 demo1: 选择酒店，选择日期房间，选择入住人数，最后确认订单;demo2: 取消最后订单 无 2 支付住宿订单 100 10 demo1: 点击支付，在第三方支付界面确认支付，支付成功；demo2: 取消支付 无 3 搜索酒店 80 10 demo1: 点击搜索框，输入酒店名demo2: 点击地标，搜索地标附近酒店demo3: 选择城市，搜索酒店demo4: 选择筛选条件，搜索酒店 无 4 收藏酒店 50 2 demo1: 进入酒店信息界面，点击收藏酒店 无 5 查看酒店交通路线图 50 10 demo1: 进入酒店信息界面，点击附近交通路线图 6 注册 50 2 demo1: 点击注册，输入注册信息，确认注册 无 7 登录 50 2 demo1: 输入账户密码，点击登录 无 2.2 业务建模 业务建模是指对商业（或非商业）组织及其运作的流程进行的建模过程。 wiki: 业务流程模型和标记法（BPMN, Business Process Model and Notation）[译注1]是工作流中特定业务流程的图形化表示法。 BPMN有以下4个基本元素： 流对象（Flow Objects）：包括事件、活动、网关，是BPMN中的核心元素； 连接对象（Connecting Objects）：包括顺序流、消息流、关联； 泳道（Swimlanes）：包括池和道两种类型； 人工信息（Artifacts）：包括数据对象、组、注释 a. 在（任务b）基础上，用活动图建模找酒店用例。简述利用流程图发现子用例的方法。 这里我对对创建住宿订单进行业务建模 寻找子用例的方法： 在每个控制流处尝试添加“是否xxx”的条件选择，以为活动图添加创新用例活动 为每个控制流中间过程做细分，尝试能否分的更细更多的子活动 b. 选择你身边的银行 ATM，用活动图描绘取款业务流程 c. 查找淘宝退货业务官方文档，使用多泳道图，表达客户、淘宝网、淘宝商家服务系统、商家等用户和系统协同完成退货业务的过程。分析客户要完成退货业务，在淘宝网上需要实现哪些系统用例 2.3 用例文本编写 用例文本三种形式：brief、casual、full 在大作业基础上，分析三种用例文本的优点和缺点 用例文本 优点 缺点 brief 简单快速 无法描述复杂用例内容 casual 编制简单，可以包含一定复杂的用例信息 无法描述过于复杂的用例内容 full 用例描述完整，详细 编制耗时长，需要多次迭代]]></content>
      <categories>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>UML</tag>
        <tag>用例建模</tag>
        <tag>backlog</tag>
        <tag>业务建模</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pytorch学习笔记]]></title>
    <url>%2F2018%2F04%2F11%2Fpytorch%2F</url>
    <content type="text"><![CDATA[1. 基本数据类型 torch.Tensor 类似tensorflow，基本数据类型是torch.Tensor，内部支持不同维度、基本类型的张量。 支持操作 支持基本的算术运算操作 numpy相互转换 调用cuda（GPU） 2. 梯度模块 autograd 用于自动计算关于Tensor算术过程中的各个梯度，是pytorch的核心库 autograd.Variable data 变量中的数据：torch.Variable grad 当前变量对某个之前变量算得的梯度 grad_fn 指向当前变量产生时所操作的函数记录，用于向前回溯找一系列操作过程 猜测：每次Tensor的变量操作会保留一个运算地址（地址里面有输入、输出、操作方法） autograd.Variable.backward() 当前变量是通过一系列的Tensor操作产生的，过程中有许多变量参与，许多变量产生 可以看做一系列Tensor操作的最终结果，当前变量当作子变量，之前参与操作变量可以看做父变量，更之前的看作是祖先变量（父变量相对于子变量而言是输入，子变量相当于输出：祖先变量—$f{运算}$—&gt;父变量—$f{运算}$—&gt;子变量） backward()发生了什么 backward()则是回溯这一系列操作，通过grad_fn依次寻找产生这个变量的父变量、祖先变量，并通过求导，求得当前子变量对父变量的导数，求子变量对祖先变量的导数（用链式法则），这个导数存储在对应的父变量、祖先变量的gra的中 在求完自变量梯度后回溯过去的 grad包含啥 父变量1的grad：$\frac {dy{子变量}} {dx{父变量1}}$ 祖先变量1的grad：$\frac {dy{子变量}} {dx{祖先变量1}}$ 3. 神经网络搭建库 torch.nn库是用于pytorch网络的搭建 torch.nn.Module 网络类型的基本父类，一般继承该类，搭建自己的网络类型 可以方便的通过Module.parameters()，在循环中访问网络中的权重变量 nn.* 网络变量类型 通过nn可以生成许多网络相关的操作算子，如：卷积核，全连接权重，池化 这些算子包括自带的数据权重（通过parameters()访问），和特定的操作函数 nn.Linear 线性权重算子：初始化权重尺寸，当输入匹配大小的数据，即可进行全连接运算 nn.Conv2d 二维卷积算子：初始化卷积核尺寸，卷积形式（padding，stride），当输入匹配大小数据，及可卷积操作 nn.MaxPool2d 二维池化算子：初始化池化尺寸，池化形式（padding，stride），输入匹配大小数据，即可池化 nn.MSELoss 平均平方差损失算子：计算loss 所有算子实例的操作都是输入一个autograd变量，输出一个autograd变量 支持batch数据输入 torch.optim自动优化权值 optim.SGD注册返回优化器（初始化优化器变量以及学习率） optim.SGD.zero_grad()清空变量梯度 optim.SGD.step()执行优化 官网实例： 12345678910111213141516171819202122232425262728293031323334353637383940import torchfrom torch.autograd import Variableimport torch.nn as nnimport torch.nn.functional as Fclass Net(nn.Module): def __init__(self): super(Net, self).__init__() # 1 input image channel, 6 output channels, 5x5 square convolution # kernel self.conv1 = nn.Conv2d(1, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) # an affine operation: y = Wx + b self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # Max pooling over a (2, 2) window x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # If the size is a square you can only specify a single number x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(-1, self.num_flat_features(x)) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] # all dimensions except the batch dimension num_features = 1 for s in size: num_features *= s return num_featuresnet = Net()print(net) 4. 分类器实战 torchvision.transforms：该模块提供多个转换器 transforms.Compose：将多个转换器串联起来 transforms.ToTensor：Convert a PIL Image or numpy.ndarray to tensor. transforms.Normalize：(mean, std)，归一化图像三个维度（C，W，H） torchvision.datasets torchvision.datasets.CIFAR10：下载数据集 torch.utils.data.DataLoader：加载数据集 torch.nn.CrossEntropyLoss 教程商误差？ —未完待续—]]></content>
      <categories>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>pytorch</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Makefile构建c++项目]]></title>
    <url>%2F2018%2F04%2F10%2FMakefile%E6%9E%84%E5%BB%BAc%2B%2B%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[做计算机视觉作业时，要用到CImg库，发现居然Xcode不能链接到CImg头文件里面引用的X11链接库，于是在网上找了各种方法。最终重新捡起Makefile搭建c++项目。 1. g++说到g++，这里我们先解释gcc： 1.1 gcc是啥 gcc : GNU CC(简称gcc)是GNU项目中符合ANSI C标准的编译系统，能够编译用C、C++、ObjectC、Jave等多种语言编写的程序。gcc又可以作为交叉编译工具，它能够在当前CPU平台上为多种不同体系结构的硬件平台开发软件，非常适合在嵌入式领域的开发编译，如常用的arm-linux-gcc交叉编译工具 参考：https://blog.csdn.net/zhaoyue007101/article/details/7699554 简而言之gcc是个编译器 1.2 gcc和g++引用一段知乎的回答： gcc 最开始的时候是 GNU C Compiler, 如你所知，就是一个c编译器。但是后来因为这个项目里边集成了更多其他不同语言的编译器，GCC就代表 the GNU Compiler Collection，所以表示一堆编译器的合集。 g++则是GCC的c++编译器。作者：李锋链接：https://www.zhihu.com/question/20940822/answer/16667772来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 1.3 gcc 参数 对于c++工程而言，gcc就是调用的g++编译器 对于普通的c工程，gcc用的不是g++编译器，由于g++编译器也会把c文件当作c++文件编译，只是链接的时候链接库不同罢了 命令格式：gcc [option] [file] 1.3.1 详细编译流程1.预处理-Pre-Processing 1gcc -E hello_world.c -o hello_world.i //.i文件 2.编译-Compiling 1gcc -S hello_world.i -o hello_world.s //.s文件 3.汇编-Assembling 1gcc -c hello_world.s -o gcc -c hello_world.s -o hello_world.o //.o文件 4.链接-Linking 1gcc hello_world.o -o hello_world //bin文件，可执行文件 1.3.2 项目惯用流程 编译 1gcc -c hello_world.c -c hello_world.o 链接 1gcc hello_world.o -c hello_world 对于c++项目是一样命令，只是gcc改成g++ 1.3.3 参数解释 大小写有区分 -E参数 -E 选项指示编译器仅对输入文件进行预处理。当这个选项被使用时, 预处理器的输出被送到标准输出而不是储 -S参数 -S 编译选项告诉 GCC 在为 C 代码产生了汇编语言文件后停止编译。 GCC 产生的汇编语言文件的缺省扩展名 -c参数 -c 选项告诉 GCC 仅把源代码编译为目标代码。缺省时 GCC 建立的目标代码文件有一个 .o 的扩展名。 -o参数 -o 编译选项来为将产生的可执行文件用指定的文件名。 -O参数 -O 选项告诉 GCC 对源代码进行基本优化。这些优化在大多数情况下都会使程序执行的更快。 -O2 选项告诉： GCC 产生尽可能小和尽可能快的代码。 如-O2，-O3，-On（n 常为0—3）； -O 主要进行跳转和延迟退栈两种优化； -O2 除了完成-O1的优化之外，还进行一些额外的调整工作，如指令调整等。 -O3 则包括循环展开和其他一些与处理特性相关的优化工作。 选项将使编译的速度比使用 -O 时慢， 但通常产生的代码执行速度会更快。 调试选项-g和-pg GCC 支持数种调试和剖析选项，常用到的是 -g 和 -pg 。 -g 选项告诉 GCC 产生能被 GNU 调试器使用的调试信息以便调试你的程序。GCC 提供了一个很多其他 C 编译器里没有的特性, 在 GCC 里你能使-g 和 -O (产生优化代码)联用。 -pg 选项告诉 GCC 在编译好的程序里加入额外的代码。运行程序时, 产生 gprof 用的剖析信息以显示你的程序的 -l参数和-L参数（前面是小写的L） -l参数就是用来指定程序要链接的库，-l参数紧接着就是库名，那么库名跟真正的库文件名有什么关系呢？-L指定库文件的所在的目录，结合-L就可以链接第三方库文件。 pthread例子：-lpthread CImg的例子：-L./src/lib/X11/lib -lX11，这个表示在#include&lt;X11&gt;的时候用./src/lib/X11/lib里的库文件libX11，一般在链接的时候用 -I（这是大写的i） 指定头文件 最后留几个问题： 交叉编译？GNU？ 链接库和引用头文件的区别 2. Makefile语法： 123target: [target1] [target2] .... command 1 command 2 target是指具体某个文件，或者是某个命令的变量名如：run 这里相当于target0需要target1和target2等依赖，必须先运行target1，target2下的命令才能运行target0 command是shell命令，这里可以直接g++ -c xxx.cpp -o xxx.o, g++ xxx.o -o xxx 待续： 文件不更新时，不会重复编译？ Makefile 的宏的高级用法 3. C++项目结构以Computer Vision的项目为例 123456789101112131415161718192021222324.├── Makefile├── build│ ├── canny.o│ └── cv_hw2.o├── docs│ ├── Ex2.docx├── images│ ├── bigben.jpg│ ├── bmp│ ├── lena.jpg│ ├── stpietro.jpg│ └── twows.jpg├── output│ ├── bigben-edge.bmp│ ├── lena-edge.bmp│ ├── stpietro-edge.bmp│ └── twows-edge.bmp└── src ├── cv_hw2 ├── cv_hw2.cpp ├── include ├── lib └── test]]></content>
      <categories>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>makefile</tag>
        <tag>g++</tag>
        <tag>gcc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络入门总结]]></title>
    <url>%2F2018%2F03%2F31%2F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[一、基础概念学习1. 感知机1.1 感知机模型 $f(x) = sign(w\cdot x+b)$ 简单判别模型，输出为正负号 1.2 感知机学习策略 通过第一范式或第二范式等形式 如第二范式损失函数（残差平方和）：$L(f) =\sum (f(x_i) - y_i)^2 =\sum (\hat {y_i}- y_i)^2$ 模型的损失函数越低，说明模型越好 1.3 感知机学习算法1.3.1 梯度的概念 即为某函数$F(x_1, x_2, …, x_n)$ 对各个变量求导后得到的向量$\overrightarrow v = [\frac {dF} {dx_1}, \frac {dF} {dx_2}, \frac {dF} {dx_3}, ….. \frac {dF} {dx_n}]$ 方向代表F在点$[x_1, x_2, … x_n]$ 指向F斜率最大的方向（F变大的方向） 1.3.2 梯度下降法 为了计算最小化损失函数，即为位于$L$的偏导为零的极小值（假设$L$是可导函数） 通过解析解求极值 使得偏导数为零，列方程组解方程，矩阵方程组求解（计算量大，容易失去精度） 通过数值解求极值 通过迭代更新规则$w = w + \eta \cdot \nabla w$， $b = b + \eta \cdot \nabla b $ $\nabla w, \nabla b$ 为梯度，$\eta $ 为学习率 注意sign不可导 2. 多层感知网络/人工神经网络（ANN） 多层感知机，中将多个感知机单元按照层与层的关系连接输入输出，使得数据在网络中的传输方向是不断向前的，也叫做前向网络；由于类似脑中的神经元拓扑结构，也叫人工神经网络 2.1 激励函数 激励函数为模型添加非线性化 多层网络中对感知机做出了改进，激励函数改用一个S型函数（sigmoid），$S(x) = \frac 1 {1 + e^{-x}}$，解决$sign(x)$不可导的问题， 2.2 网络结构 一层由多个感知机单元组成，上一层结构的所有感知机的输出将输入当前层每个神经元 即为每个神经元需要以上一层所有神经元的输出作为输入，所以也就全连接 输入层+隐藏层+输出层 对于多分类问题，可能结尾会需要使用一个softmax函数转换为概率分布 2.3 反向传播算法由于出现了多层网络，就出现一个问题，原来的梯度下降法是否用于模型的优化呢？ 答案是可以的，在BP的论文中给予了证明，即通过链式法则求各层各个参数的偏导数。 BP的核心指出，在网络中下一层的得到的梯度，可以在上一层梯度计算的过程中使用，所以各层的梯度计算是一个递推的过程，所以每层的时间复杂度都是相同的，指出多层网络中的梯度计算是可行的 由于梯度的计算是从网络的输出层计算到输入层，与前向网络的方向（输入层到输出层）相反，同时由于梯度的计算是层次相关的，类似传播的过程，所以叫做反向传播算法 3. 卷积神经网络（CNN） 与ANN不同，引入卷积概念，池化概念，ReLU概念 3.1 卷积的概念 feature map：指数据的形式 feature map size：$c \times w \times h$ (频道(channel) x 宽 x 高) kernel/mask：卷积核指的是一个三维的张量 kernel size：$d\times w \times h$ （深度 x 宽 x 高） receptive field: 一个kernel与feature map一次卷积的feature map的范围（一次卷积只能用一部分feature map，计算得到一个值） padding：原始feature map卷积前的边缘扩充 stride：kernel卷积中移动的距离 convolution：通过卷积核在原feature map上各个感知域（receptive field）点乘求和操作，提取feature map的特征 convolution 使用的是 3.2 池化的概念 最大池化 平均池化 pooling 是一种下取样的方法，目的是为了降低feature map的尺度 3.3 用ReLU代替激励函数sigmoidAlexNet中提出，效果得到优化 相当于去了个绝对值，同样是非线性，在x = 0处不可导 3.4 网络结构与ANN一样是：输入层+隐藏层+输出层 不同于加入了卷积层，池化层，ReLU方法。 卷积层一般紧接池化层 二、代表架构学习 代表架构的学习让我学到了除网络结构以外的东西 一些数据预处理的方法，如AlexNet中数据增强 解决过拟合的方法，如Dropout，LRN（local response normalization） 各种新型网络的结构，以及好的CNN模型的特点（深、窄） CNN模型的可适用领域广泛，各种解决各种其他实际问题 了解了一些有趣的算法，selective searching RoadMap LeNet 首次提出卷积网络应用于人工神经网络 提出池化层的概念 AlexNet 改进池化层，使用最大化池 使用ReLU代替sigmoid 为了防止过拟合，提出Dropout，Data Augmentation 提出LNR（local response nomarlization） 运用GPU的并行运算的性能 ZFNet 对AlexNet提出可视化层面研究，研究CNN背后机理 提出解卷积层，可视化CNN学习的过程 减小AlexNet部分卷积核，减少参数得到更好效果 VGG 研究CNN不同模型的性能，提出更深的网络以及更小的卷积核的改进思路 提出3x3卷积核，简化模型，使得需要训练的参数更少 feature map随着尺寸变小，逐渐加深层数 GoogLeNet 提出了Inception module Inception module中运用许多并行结构 参量减少更多 可以运用于图像检测 ResNet 为解决层数太多，梯度消失的问题，提出残差模型 下面是关于图像检测的模型，许多只是在粗略的学习 R-CNN 第一阶段通过selective searching 算法进行Region Proposal 区域预选，每个图片提取2000个regions 第二阶段通过CNN提取每个region的特征 第三阶段通过SVM线性分类器，对每个region的类型进行识别，同时设置阈值，去除无信息regions 后期做box-bounding regression 使得提取区域的边界更为准确 Fast R-CNN 保留selective searching 将SVM部分去除 提出RoI，以及RoI池化层，用于训练box bounding CNN改造成两个部分，一个部分用来训练分类，一个用于训练box bounding 一定程度上增强了CNN网络的作用，以及简化了模型 三、工程学习Tensorflow 搭建LeNet的简化模型，训练手写体，在测试集中达到超过94%的正确率 下阶段打算尝试搭建VGG，ResNet，训练相片分类，熟悉网络的搭建 Pytorch👉http://blog.zhanzy.xyz/2018/04/11/pytorch/]]></content>
      <categories>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发模型对比]]></title>
    <url>%2F2018%2F03%2F17%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[1. 简答题1.1 简述瀑布模型、增量模型、螺旋模型（含原型方法）的优缺点。 模型 优点 缺点 瀑布模型 - 定义软件开发基本流程与活动- 降低软件开发的复杂程度，提供软件开发过程的透明性，提供软件开发过程的可管理性- 推迟软件实现，强调软件实现前必须进行分析和设计工作- 以项目的阶段评审和文档控制为手段有效的对整个开发过程进行指导，保证了阶段之间的正确衔接，能够及时发现并纠正开发过程中存在的缺陷，使产品达到预期的质量要求 - 依赖问题：前面需求模糊，后面工作出现问题- 容错问题：在后期发现需求问题，工作量难以接受- 资源调配问题：只是技能需求不同，人员数量要求不同- 强调过程活动的线性顺序- 缺乏灵活性，特别是无法解决软件需求不明确或不准确的问题- 风险控制能力弱- 瀑布模型中的软件活动是文档驱动的，当阶段之间规定过多的文档是，会极大增加系统工作量- 管理人员如果仅仅以文档的完成情况来评估项目的完成进度，往往会产生错误的结论 增量模型 - 增强客户对系统的信心- 降低系统失败风险- 提高系统可靠性- 提高系统稳定性和可维护性 - 增量粒度难以选择- 确定所有的基本业务服务比较困难 螺旋模型（含原型方法） - 可以很好地接受需求的改变- 使用和拓展原型方法- 需求可以被更准确的分析- 使用者可以很早看到系统的原型，以此做出早期的评估- 开发过程可以被分为很小的部分，风险在早期做出评估，达到更好的风险管理 - 管理过程是复杂的- 项目的结束难以预估- 不太适合小项目- 风险分析需要相当的成本耗费，只适合投资较大的规模软件项目- 大量的中间阶段产生大量的文档- 失误的风险分析可能带来更大的风险 1.2 简述 UP 的三大特点，其中哪些内容体现了用户驱动的开发，哪些内容体现风险驱动的开发？ 三大特点 1. 迭代增量的（Iterative and Evolutionary）： 工程初始阶段的知识是不完全的，不全面的。这种方式的优点在于：在初始阶段后逐步趋向稳定；有效的管理需求的变换；持续集成；尽早接触整个系统；在线风险评估2. 架构为中心的（Architecture Centric）： 架构讨论关于软件系统总体结构、结构元素之间的接口、结构元素之间的合作以及他们的行为来表示。架构为中心指的是围绕某个软件架构为中心进行开发，架构为中心勾勒出了一系统概貌，提供一套成熟的有组织的框架使用，有利于重用3. 案例驱动的（Use Case Driven）： 案例是指，一个时期被分为一系列动作，一个动作同时有很多个参与者参与，这些动作的共同结果是可视化的，帮助参与者更好的达到他们的目标。案例驱动指，开发团队的开发，从需求分析到编码和测试都是应用案例的方法 用户驱动的开发 案例驱动开发：通过案例的方法，使得每个阶段的内容可视化，有助于协调用户明确需求迭代增量的开发：尽早接触整个系统，使得成功可以看见 风险驱动的开发 迭代增量的开发：有助于风险评估架构为中心开发：使用现有的开发架构有利于降低风险性，提高开发效率 1.3 UP 四个阶段的划分准则是什么？关键的里程碑是什么？四个阶段的划分准则是：依据各个阶段能达到的成果或者是目标 阶段 成果或目标 里程碑 初始阶段（Inception） 1. 为系统建立业务案例，确定项目边界- 业务案例：项目验收规范、风险评估、所需资源估计、阶段计划- 确定项目边界：识别所以与系统交互的外部实体、定义外部是体育系统的交互特性（识别外部角色、识别所以用力并详细描述一些重要的用例） （生命周期目标里程碑的相关文档）1. 项目构想文档2. 原型用例模型3. 原始业务风险评估4. 一个或者多个原型5. 原始业务案例 精化阶段（Elaboration） 1. 分析问题领域2. 建立健全的体系结构基础3. 编制项目计划4. 完成项目中高风险需求部分的开发 （生命周期体系结构里程碑）1. 风险分析文档2. 软件体系结构基准3. 项目计划4. 可执行的进化原型5. 初始版本的用户手册 构建阶段（Construction） 1. 完成所有剩余的技术构件2. 稳定业务需求功能的开发3. 集成为产品4. 详细测试所有功能 （执行功能里程碑）1. 可运行的软件产品2. 用户手册 交付阶段（Transition） 1. 确保软件对最终用户是可用的- 为发布准备的产品测试- 基于用户反馈的少量调整 （产品发布里程碑） 1.4 IT 项目管理中，“工期、质量、范围/内容” 三个元素中，在合同固定条件下，为什么说“范围/内容”是项目团队是易于控制的“范围/内容”的控制与项目管理中的要素成本的控制有关，范围/内容越大所需工程成本越高，而在合同固定的条件下，合同不能随意更改，说明工程项目的客户需求是固定的，对于项目的开发而言代表项目前期的需求确定，以及甲方提供成本是确定的。 所以对于有经验的项目团队，由于过往项目的完成经验，可以根据合同需求，以及成本，分析项目合理的范围。所以，这对于项目团队是易于控制的。 1.5 为什么说，UP 为企业按固定节奏生产、固定周期发布软件产品提供了依据？UP的软件生命周期从时间上分为四个阶段，每个阶段包括一个主要的里程碑。阶段是两个主要里程碑的分隔，在各个阶段结束时，执行评估阶段目标是否满足以决定是否进入下一个阶段。因此RUP提供了固定节奏的生产。UP是一个风险驱动的生命周期模型，为了有效地控制风险，UP以渐进的方式进行演进，首先解决高风险的问题，这主要是通过迭代来实现。在软件生命周期中，每个阶段可以划分为多个迭代，每个迭代确定一个内部里程碑（或一个发布）。因此，UP也为固定周期发布软件产品提供了依据。 2. 项目管理使用 使用截图工具（png格式输出），展现你团队的任务 Kanban，请注意以下要求 每个人的任务是明确的。即一周后可以看到具体成果 每个人的任务是1-2项。 至少包含一个团队活动任务 Our Kanban]]></content>
      <categories>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>软件开发模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件工程关键词概览]]></title>
    <url>%2F2018%2F03%2F09%2F%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E5%85%B3%E9%94%AE%E8%AF%8D%E6%A6%82%E8%A7%88%2F</url>
    <content type="text"><![CDATA[1. 简单题1.1 软件工程的定义答： 将系统化、规范化、可度量化应用于软件的开发、运行、维护的过程，即将工程化应用于软件中 对1中所描述的方法的研究 1.2 阅读经典名著“人月神话”等资料，解释software crisis、COCOMO模型答： software crisis: 指落后的软件生产方式无法满足软件日益增长的需求，导致软件开发、运行、维护过程中出现一系列严重问题，主要有如下几种问题 软件开发的成本日益增长 软件开发进度难以控制 用户对”已完成“系统不满意的现象经常发生 软件产品的质量不可靠 软件的可维护程度低 软件开发生产率跟不上硬件的发展和人们需求的增长 COCOMO模型 COCOMO模型：结构性成本模型，是一种精确的、易于使用的软件成本估算方法。 这种模型使用一种基本的回归分析，使用从项目历史和现状中的某些特征作为参数来进行计算。 可以分为三个层次： 基本COCOMO模型 基本COCOMO是一种静态的单值模型，它使用以每千源代码行数（KLoC）来度量的程序大小来计算软件开发的工作量（及成本） 中级COCOMO模型 中级COCOMO对软件工作量的估算使用了程度大小以及一组“成本驱动者”，包括对产品、硬件、人员及项目属性的客观评价 详细COCOMO模型 详细的COCOMO整合了中间版本的所有特性，并评估了成本动因对软件工程过程的每个步骤（分析，设计等）的影响。 1.3 软件生命周期答： 把软件的开发、运行、维护过程分段 4个时期、7个阶段： 软件分析时期：问题定义、可行性研究、需求分析 软件设计时期：总体设计、详细设计 编码与测试时期：编码、测试 运行与维护时期 1.4 按照SWEBok的KA划分，本课程关注哪些KA或知识领域答： 软件需求 Software Requirements 软件设计 Software Design 软件构建 Software Construction 软件工程模型和方法 Software Engineering Models and Methods 1.5 解释CMMI的五个级别。例如：Level 1 - Initial：无须，自发生产模式答： Level 1 - Initial 初始级：软件过程是无序的，对过程几乎没有定义，成功与否取决于个人努力，管理是反应式的 Level 2 - Managed 管理级：开发过建立项目管理过程监督费用、进度和功能特性。制定必要过程纪律，可以重复早些的类似项目成功经理 Level 3 - Defined 定义级：已经量软件管理和工程两方面的过程文档化、标准化，并综合成组织软件的标准软件过程。所以项目均通过批准的标准软件过程来开发和维护，软件产品的生产在软件开发过程是可见的 Level 4 - Quantitatively Managed 量化管理级： 分析软件过程和产品质量的详细度量数据，对软件过程和产品都有定量的理解和控制。管理有一个做出结论的客观依据，管理能够在定量的范围内预测性能 Level 5 - Optimizing 优化管理级：量化反馈和先进的新思想、新技术促使过程不断改进 1.6 用自己语言简述SWEBok或CMMI（约200字）答： SWEBok 指software engineering body of knowledge，即软件工程体系知识 SWEBok是一个国际标准，定义这个国际标准的目的是为了：定义软件工程标准的内容，促成世界对软件工程的一致观点，定义软件工程的学科领域与其他学科的不同，为软件工程的教材和课程内容提供基础，为软件工程师的认证和授权提供基础 SWEBok将软件工程体系知识定义为15个KA（knowledge area）知识域，并且为各个知识域定义了详细的内容 15个知识域可以分为两个方面的内容，一个是软件工程实践知识域，一个是软件工程必备知识基础 2. 解释PSP各项指标及技能要求 阅读《现代软件工程》的 PSP: Personal Software Process 章节。 http://www.cnblogs.com/xinz/archive/2011/11/27/2265425.html 按表格 PSP 2.1， 了解一个软件工程师在接到一个任务之后要做什么，需要哪些技能，解释你打算如何统计每项数据？ （期末考核，每人按开发阶段提交这个表） 答： 2.2 工作内容和所需技能 工作内容 所需技能 计划 - - 估计这个任务需要多少时间 多年工作经验，对工作量的合理评估 开发 基本编程能力 - 分析需求 明确用户需求能力 - 生成设计文档 设计软件结构的能力，相关设计工具的使用能力 - 设计复审 (和同事审核设计文档) 合作能力，软件设计规范 - 代码规范 (为目前的开发制定合适的规范) 契合团队开发代码风格 - 具体设计 编码能力 - 代码复审 - 测试（包括自我测试，修改代码，提交修改） 测试工具的使用能力 记录时间花费 测试报告 报告编写 计算工作量 事后总结 自我反省的能力 提出过程改进计划 完善自我的能力 2.2 怎样做记录 新建一个表格文件，作为记录各个工作开始时间的日志 将上述各项工作填入表中 每当表中某项任务开始的时候记录开始的时间 在项目最后统计用时（通过统计每项工作在每天工作时间段中的用时）]]></content>
      <categories>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>软件工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shadowsocks-libev+CentOS 7 搭梯]]></title>
    <url>%2F2018%2F02%2F01%2Fss-libev%2F</url>
    <content type="text"><![CDATA[一、VPS 供应商：Vultr、DigitalOcean Vultr 优点： 节点位置多 提供节点测速 邀请码优惠：老推新，新人享10刀 最低消费：2.5 dollars per month（虽然都显示告罄，还是有5刀的可以选） 可以支付宝 缺点： 首次必须充值最低10刀 个人经验： 不要相信别人说的亚洲节点好，其实在Vultr里亚洲节点最不稳定 实际性能还是要自己测试还是需要自己测试 尽量一个节点多测试几次，一个是看时延长短，一个是看时延波动情况 Silicon Valley（硅谷）节点时延几乎最低，最稳定，稳定在170+ms 新人通过老用户的链接获取的10刀必须在账户首次充值后有效，首次充值最低10刀，所以天下没有免费的午餐 DigitalOcean 优点： 邀请码：老推新，新人享10刀 GitHub学生认证大礼包特惠码：50刀（各种学生优惠大礼包） 可以支付宝 首次必须充值最低5刀 缺点： 几乎所有节点都是延时250+ms，除了旧金山节点可以进到200ms内，但是不稳定 及其不稳定，一个节点的时延波动极大，实测可能还会丢包 一旦一个邀请码或特惠码使用后不能用第二个特惠码，即老用户邀请的新用户无法使用更GitHub学生大礼包的50刀 VPS系统内核升级很难 个人经验： 不是很推荐，除了50刀诱惑人（可以用一年） 我可是连续开了三个号才弄清GitHub学生大礼包的优惠码不能喝邀请码同时使用 系统选择 尝试了CentOS 7，“据说”比较稳定。 总结 偏向于Vultr，便宜而且部分节点很稳。DigitalOcean除了有大礼包吸引我，就没别的了🤦‍♀️ 个人推荐1：Vultr + CentOS 7 + Silicon Valley 个人推荐2：Vultr (CentOS 7 + Silicon Valley) + DigitalOcean (CentOS 7 + San Francisco) 由于可能容易被墙识别然后block，就用大礼包的免费50刀开个备用线路 二、CentOS 7 + shadowsocks-libev 集各家之精华 Reference： shadowsocks.org GitHub CentOS 7安装ShadowSocks（libev版本）加上KCP加速 CentOS 7 配置 shadowsocks-libev 服务器端进行科学上网 CentOS 7 下安装 Shadowsocks 服务端 CentOS 搭建 Shadowsocks-libev 环境 1. 更新基本编译依赖123# yum install epel-release -y# yum update# yum install gcc gettext autoconf libtool automake make openssl-devel pcre-devel asciidoc xmlto zlib-devel openssl-devel libsodium-devel udns-devel libev-devel -y 2. 安装添加单独的仓库，可能要另外安装wgetyum install wget。 完成下面命令/etc/yum.repos.d/目录下出现librehat-shadowsocks-epel-7.repo 12# wget https://copr.fedorainfracloud.org/coprs/librehat/shadowsocks/repo/epel-7/librehat-shadowsocks-epel-7.repo# cp librehat-shadowsocks-epel-7.repo /etc/yum.repos.d/ 安装shadowsocks-libev 12# yum update# yum install shadowsocks-libev 如果出现： No package udns-devel available.No package mbedtls-devel available. 尝试 1234&gt; # yum -y install yum-utils&gt; # yum-config-manager --enable epel&gt; # yum update&gt; 3. 编辑配置文件编辑配置文件/etc/shadowsocks-libev/config.json 12345678910&#123; &quot;server&quot;: &quot;34.24.24.241&quot;, &quot;server_port&quot;: 43943, &quot;local_address&quot;: &quot;127.0.0.1&quot;, &quot;local_port&quot;: 1080, &quot;password&quot;: &quot;zzz.buzz&quot;, &quot;method&quot;:&quot;aes-256-gcm&quot;, &quot;timeout&quot;: 600 &quot;mode&quot;: &quot;tcp_and_udp&quot;&#125; 加密推荐aes-256-gcm，尽量不要和网上样选aes-256-cfb 端口推荐五位，不要常用端口如：8888，6666，2333 上面措施都是为了防止被墙学习到，然后导致Connection has been reset by perr.（其实可能也没什么用。。） 4. 启动服务器这里用运行系统服务的方式（其实也可以直接ss-server启动） 系统服务的方式缺点就是看不到各种进过服务器流量的日志和错误日志。 123# systemctl start shadowsocks-libev# systemctl status shadowsocks-libev -l# systemctl stop shadowsocks-libev 将该服务放入系统默认启动服务 1# systemctl enable shadowsocks-libev 下面几句查看状态，错误时可以查看（虽然作为服务可以看到的log信息不多） 12345678# 如果遇到故障，可以查看最近一次启动后的系统日志：# journalctl -b# 或者指定查看 shadowsocks 最近一次启动后的日志：# journalctl -b -u shadowsocks# 或者实施跟踪最近一次启动后的实时日志：# journalctl -f -b -u shadowsocks 顺便查看系统服务 123# systemctl list-units –type=serviceor# systemctl list-unit-files | grep enabled 5. 客户端安装自己用的是gui客户端：ShadowsocksX-NG 其他版本 三、Firewalls iptables iptables和iptables-services不一定同时都有，iptables-services装了才能当作服务运行 下面第三句保存过滤规则到系统服务中 123# yum install iptables iptables-services -y# iptables -A INPUT -p tcp -m state --state NEW -m tcp --dport [server port] -j ACCEPT# service iptables save 启动服务，注册到开机基本服务中，使得开机自启 12# systemctl start iptables.service# systemctl enable iptables.service 熟悉的几句，查看状态 1# systemctl status iptables.service 这几句同样奏效 12345&gt; # service iptables start&gt; # service iptables status&gt; # service iptables stop&gt; # service iptables restart&gt; firewalld 没用到╮(╯_╰)╭ ServerSpeeder/ BBR/ Kcptun/ FinalSpeed ServerSpeeder锐速 系统内核需要在级别kernel-3.10.0-229.1.2.el7.x86_64.rpm 12# rpm -ivh http://soft.91yun.org/ISO/Linux/CentOS/kernel/kernel-3.10.0-229.1.2.el7.x86_64.rpm --force# reboot 安装 1#wget -N --no-check-certificate https://github.com/91yun/serverspeeder/raw/master/serverspeeder.sh &amp;&amp; bash serverspeeder.sh 使用 12# service serverSpeeder start# service serverSpeeder status BBR linux内核4.9+版本自带，有需要则可以开启 Kcptun 服务端配置简单一条龙服务 客户端结合ss客户端使用麻烦 windows gui FinalSpeed 没有尝试 SSR混淆弄了很久，是在搞不动。。]]></content>
      <categories>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>shadowsocks</tag>
        <tag>VPS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[这样的我真的很颓、很废、很没脾气]]></title>
    <url>%2F2018%2F01%2F24%2F20180124%2F</url>
    <content type="text"><![CDATA[早 “勤劳”的蜜蜂睡在阴暗的蜂巢里 他不想出去 其他的蜂都去采蜜了 他也想 可是他从来不知道去哪里采，从哪开始采 呼吸着充满蜜香、而又浑浊的空气 他再次不安稳的昏睡过去 一个浑浑噩噩的早上没了 ”还没醒啊“ 午 我清醒了 盯着镜子里的我 ：“在干嘛” ：“学习” 十分钟后 ：“在干嘛” ：“学习” 十分钟后 … 看着桌面的一切，回想着下午 突然感到害怕 原来不再是下午了啊 我怀念那个不清醒的早晨了 回想自己所做的，总觉得不充实，总觉得在浪费时间 无头苍蝇，闻到哪香，就往哪飞 晚 我们用一张纸描绘我们的藏宝图 你用一双手把它揉成了团 我吞下了那团纸 再也没见过你 那样一定是很痛苦啊 开心的去找那个女孩 … 不开心的回来了 她也不开心了 不想看到我 我也不想看到我]]></content>
      <categories>
        <category>记录内心</category>
      </categories>
      <tags>
        <tag>丧</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一篇博文：Hello Hexo]]></title>
    <url>%2F2017%2F09%2F26%2Fhello-hexo%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment hexo配置相关：http://theme-next.iissnan.com/getting-started.html]]></content>
      <categories>
        <category>计算机科学</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>前端</tag>
      </tags>
  </entry>
</search>
