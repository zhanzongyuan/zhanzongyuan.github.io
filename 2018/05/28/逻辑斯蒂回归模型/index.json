{"summary":"<p>逻辑斯蒂分布函数：$F(X\\leq x) = \\frac 1 {1 + e^{\\frac {x-\\mu } {\\gamma} }}$ </p>\n<p><img src=\"/2018/05/28/逻辑斯蒂回归模型/Screen Shot 2018-05-29 at 3.52.27 PM.png\" alt=\"Screen Shot 2018-05-29 at 3.52.27 PM\"></p>\n<a id=\"more\"></a>\n<h2 id=\"一、二项逻辑斯蒂回归模型\"><a href=\"#一、二项逻辑斯蒂回归模型\" class=\"headerlink\" title=\"一、二项逻辑斯蒂回归模型\"></a>一、二项逻辑斯蒂回归模型</h2><h3 id=\"1-1-概率角度\"><a href=\"#1-1-概率角度\" class=\"headerlink\" title=\"1.1 概率角度\"></a>1.1 概率角度</h3><p>二项逻辑斯蒂回归模型是如下条件概率分布：</p>\n<script type=\"math/tex; mode=display\">\nP(Y=0| x) = \\frac 1 {1 + exp(w·x + b)} \\\\\nP(Y=1| x) = \\frac {w·x + b} {1 + exp(w·x+b)} \\\\\nx \\in R^n ,\\ Y \\in \\{0, 1\\} ,\\ w \\in R^n ,\\ b \\in R</script><p>这里的$x$是输入，$Y$是输出，$w, b$是训练变量</p>\n<h3 id=\"1-2-回归角度\"><a href=\"#1-2-回归角度\" class=\"headerlink\" title=\"1.2 回归角度\"></a>1.2 回归角度</h3><blockquote>\n<p> 普通的线性模型适用于线性回归问题</p>\n<p>当遇到分类问题时，相当于线性回归的输出值域变为${0, 1}$，所以单位阶跃函数可以说是一个理想的选择，但是这个函数并不是可导的，所以在求解最优模型的过程中，我们无法使用梯度下降的方法</p>\n<p>所以就用一个形状类似”S”的函数来近似单位阶跃函数，用这个函数将值域映射到$[0, 1]$，这是个连续的区间。</p>\n<p>所以就使用了对数几率函数$g(z) = \\frac 1 {1+e^{-z}}$ </p>\n</blockquote>\n<p>二项逻辑斯蒂回归模型是一个预测样本$x$标签$y\\in[0,1]$的函数，弥补了线性函数在二分类问题上的不足，将原本$w·x+b \\in R$的线性模型值域，映射到$[0, 1]$这个值域范围：</p>\n<script type=\"math/tex; mode=display\">\nh_\\theta(x) = g(\\theta^T x)  ,\\ \\text {这里x是齐次坐标，} \\theta^T x \\text {相当于} w·x+b\\\\\ng(z) = \\frac 1 {1+e^{-z}}</script><p>所以有：$h_{\\theta}(x) = \\frac 1 {1 + e ^{- \\theta^Tx}}$ </p>\n<p><br></p>\n<p><strong>转化为线性回归问题</strong>：</p>\n<p>曾经尝试过通过解析几何的方式，解逻辑斯蒂回归模型解析解</p>\n<p>令$h_\\theta (x) = y$ ，则可以得到 $ ln \\frac y {1-y} = \\theta^T x$ 这个形式，这样就可以列矩阵方程组解$\\theta$的值，其中需要将标注由0 映射为0.0001， 由1映射为0.9999，由于分母不能为零，相当于把标签化为概率。实际上这样操作得到的精度不高</p>\n<p><br></p>\n<p><strong>对数几率</strong>：上面出现的 $ln \\frac y {1-y}$，如果将y视为x为正样本的概率，则$\\frac y {1-y}$就是几率，而$ln \\frac y {1-y}$就是对数几率也叫logit。从这个角度看，也是相当于把原本二值标签通过对数几率映射到$R$ ，使得用线性回归拟合对数几率。（所以是对对数几率的回归，也叫logit regression）</p>\n<h2 id=\"二、模型选择的策略\"><a href=\"#二、模型选择的策略\" class=\"headerlink\" title=\"二、模型选择的策略\"></a>二、模型选择的策略</h2><blockquote>\n<p>上面的内容提出了逻辑斯蒂回归模型，对于同一个逻辑斯蒂回归模型，有无数种可能的$w, b, (\\theta)$ ，所以要对模型$h_{w_i, b_i}$ 做出评价，评价这个模型的好坏，才能进入模型的学习阶段。这个评价的准则，就是一种模型选择的策略</p>\n</blockquote>\n<p>通过把模型输出$y$看作是正样本的概率，如1.1中的模型角度。为解这个概率模型，我们选择”极大似然法“，对模型中参数评价好坏。</p>\n<p>模型中，每个样本$x<em>i$的正样本概率：$p(y_i = 1 | x_i) = h</em>\\theta(x<em>i)$ ，负样本概率$p(y_i = 0| x_i) = 1-h</em>\\theta(x_i)$ </p>\n<p>所以极大似然估计，模型中参数为$w, b$的可能性为：$l(w, b) = \\prod [y·p(y_i = 1| x_i) + (1-y)·p(y_i = 0 | x_i)]$ ，其中$y·p(y_i = 1| x_i) + (1-y)·p(y_i = 0 | x_i)$是似然项。一般会化成对数似然函数求解$max\\ l(w, b)$ ：</p>\n<script type=\"math/tex; mode=display\">\nl(w, b) = \\sum log(y·p(y_i = 1| x_i) + (1-y)·p(y_i = 0 | x_i))</script><p>所以这个对数似然函数即为我们的策略：$l(w, b)$，希望最大化对数似然函数</p>\n<p><br></p>\n<p><strong>损失函数</strong>：</p>\n<p>在通常情况下，我们偏向于将模型选择的策略视为一个模型的损失函数，并且希望这个损失函数越小越好。所以为了上述对数似然函数安装损失函数的方式理解，我们加入负号：把最大化对数似然函数任务转换成最小化损失函数$l(w, b) =-\\sum log(y·p(y_i = 1| x_i) + (1-y)·p(y_i = 0|x_i))$ </p>\n<p>py老师的机器学习课程上，将其表述为：$J(\\theta) = - \\frac 1 m \\sum<em>{i=1}^m [y^{(i)}log h</em>\\theta(x^{(i)}) + (1-y^{(i)} log (1 - h_\\theta(x^{(i)})))$ ，本质上是一样的</p>\n<h2 id=\"三、学习的算法\"><a href=\"#三、学习的算法\" class=\"headerlink\" title=\"三、学习的算法\"></a>三、学习的算法</h2><blockquote>\n<p>有了模型（假设空间），有了策略（模型评估），现在需要的就是用一套算法去学习出评价最优的模型</p>\n<p>通过上述两点的总结，这里我们把模型的学习算法目标归结为最小化损失函数的最优化问题</p>\n<p>按照最优化的观点看，目标函数（损失函数）是凸函数，因此多种优化算法都能够适用，保证可以找到全局最优解。常见算法有：梯度下降法（SGD）、迭代尺度法（IIS）、牛顿法（BGFS）或拟牛顿法（L-BGFS）。牛顿法或拟牛顿法一般收敛速度更快。</p>\n</blockquote>\n<p>这里我们探讨梯度下降算法，核心公式：</p>\n<script type=\"math/tex; mode=display\">\n\\theta_{i+1} = \\theta_i - \\alpha \\frac {\\partial J(\\theta)} {\\partial \\theta_j}</script><p>难度不大，只需要对$J(\\theta)$求各维参数的导数即可：</p>\n<script type=\"math/tex; mode=display\">\n\\frac {\\partial J(\\theta)} {\\partial \\theta_j} = \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}</script><h2 id=\"四、正则项\"><a href=\"#四、正则项\" class=\"headerlink\" title=\"四、正则项\"></a>四、正则项</h2><blockquote>\n<p>引入正则化项的目的是为了防止过拟合问题。</p>\n<p>根据奥卡姆剃刀原则，解决同样的问题的模型中，尽量选择简单的模型</p>\n<p>正则化项就是为了降低LR模型中系数的复杂度</p>\n</blockquote>\n<p>解决过拟合的两种方法：</p>\n<ol>\n<li>降低特征的数量<ol>\n<li>手工筛选特征</li>\n<li>模型选择算法</li>\n</ol>\n</li>\n<li>正则化<ol>\n<li>保留特征，但是减小特征的大小</li>\n<li>效果很好，如果我们有很多特征，保留每个特征都能为预测y做贡献</li>\n</ol>\n</li>\n</ol>\n<h3 id=\"4-1-为-J-theta-引入正则化项\"><a href=\"#4-1-为-J-theta-引入正则化项\" class=\"headerlink\" title=\"4.1 为$J(\\theta)$引入正则化项\"></a>4.1 为$J(\\theta)$引入正则化项</h3><p><img src=\"/2018/05/28/逻辑斯蒂回归模型/Screen Shot 2018-06-01 at 12.44.25 AM.png\" alt=\"Screen Shot 2018-06-01 at 12.44.25 AM\"></p>\n<p>这样的正则化项迫使得模型的参数$\\theta$尽可能的小。为此我们还需要重新求导，重新计算梯度公式：</p>\n<p><img src=\"/2018/05/28/逻辑斯蒂回归模型/Screen Shot 2018-06-01 at 12.49.51 AM.png\" alt=\"Screen Shot 2018-06-01 at 12.49.51 AM\"></p>\n"}