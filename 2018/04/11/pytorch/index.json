{"summary":"<h2 id=\"1-基本数据类型\"><a href=\"#1-基本数据类型\" class=\"headerlink\" title=\"1. 基本数据类型\"></a>1. 基本数据类型</h2><ol>\n<li><strong>torch.Tensor</strong></li>\n</ol>\n<p>类似tensorflow，基本数据类型是torch.Tensor，内部支持不同维度、基本类型的张量。</p>\n<ul>\n<li>支持操作<ul>\n<li>支持基本的算术运算操作</li>\n<li>numpy相互转换</li>\n<li>调用cuda（GPU）</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"2-梯度模块\"><a href=\"#2-梯度模块\" class=\"headerlink\" title=\"2. 梯度模块\"></a>2. 梯度模块</h2><ol>\n<li><strong>autograd</strong></li>\n</ol>\n<p>用于自动计算关于Tensor算术过程中的各个梯度，是pytorch的核心库</p>\n<ol>\n<li><strong>autograd.Variable</strong></li>\n</ol>\n<p><img src=\"/2018/04/11/pytorch/Variable.png\" alt=\"Variable\"></p>\n<ul>\n<li>data<ul>\n<li>变量中的数据：torch.Variable</li>\n</ul>\n</li>\n<li>grad<ul>\n<li>当前变量对某个之前变量算得的梯度</li>\n</ul>\n</li>\n<li>grad_fn<ul>\n<li>指向当前变量产生时所操作的函数记录，用于向前回溯找一系列操作过程</li>\n<li>猜测：每次Tensor的变量操作会保留一个运算地址（地址里面有输入、输出、操作方法）</li>\n</ul>\n</li>\n</ul>\n<ol>\n<li><strong>autograd.Variable.backward()</strong></li>\n</ol>\n<blockquote>\n<p>当前变量是通过一系列的Tensor操作产生的，过程中有许多变量参与，许多变量产生</p>\n<p>可以看做一系列Tensor操作的最终结果，当前变量当作子变量，之前参与操作变量可以看做父变量，更之前的看作是祖先变量（父变量相对于子变量而言是输入，子变量相当于输出：祖先变量—$f<em>{运算}$—&gt;父变量—$f</em>{运算}$—&gt;子变量）</p>\n</blockquote>\n<ul>\n<li>backward()发生了什么<ul>\n<li>backward()则是回溯这一系列操作，通过grad_fn依次寻找产生这个变量的父变量、祖先变量，并通过求导，求得当前子变量对父变量的导数，求子变量对祖先变量的导数（用链式法则），这个导数存储在对应的父变量、祖先变量的gra的中</li>\n<li>在求完自变量梯度后回溯过去的</li>\n</ul>\n</li>\n<li>grad包含啥<ul>\n<li><em>父变量1</em>的grad：$\\frac {dy<em>{子变量}} {dx</em>{父变量1}}$</li>\n<li><em>祖先变量1</em>的grad：$\\frac {dy<em>{子变量}} {dx</em>{祖先变量1}}$</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"3-神经网络搭建库\"><a href=\"#3-神经网络搭建库\" class=\"headerlink\" title=\"3. 神经网络搭建库\"></a>3. 神经网络搭建库</h2><blockquote>\n<p> torch.nn库是用于pytorch网络的搭建</p>\n</blockquote>\n<ol>\n<li><strong>torch.nn.Module</strong></li>\n</ol>\n<p>网络类型的基本父类，一般继承该类，搭建自己的网络类型</p>\n<p>可以方便的通过Module.parameters()，在循环中访问网络中的权重变量</p>\n<ol>\n<li><strong>nn.* 网络变量类型</strong></li>\n</ol>\n<p>通过nn可以生成许多网络相关的操作算子，如：卷积核，全连接权重，池化</p>\n<p>这些算子包括自带的数据权重（通过parameters()访问），和特定的操作函数</p>\n<ul>\n<li>nn.Linear<ul>\n<li>线性权重算子：初始化权重尺寸，当输入匹配大小的数据，即可进行全连接运算</li>\n</ul>\n</li>\n<li>nn.Conv2d<ul>\n<li>二维卷积算子：初始化卷积核尺寸，卷积形式（padding，stride），当输入匹配大小数据，及可卷积操作</li>\n</ul>\n</li>\n<li>nn.MaxPool2d<ul>\n<li>二维池化算子：初始化池化尺寸，池化形式（padding，stride），输入匹配大小数据，即可池化</li>\n</ul>\n</li>\n<li>nn.MSELoss<ul>\n<li>平均平方差损失算子：计算loss</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p> 所有算子实例的操作都是输入一个autograd变量，输出一个autograd变量</p>\n<p>支持batch数据输入</p>\n</blockquote>\n<ol>\n<li>torch.optim自动优化权值</li>\n</ol>\n<ul>\n<li>optim.SGD注册返回优化器（初始化优化器变量以及学习率）</li>\n<li>optim.SGD.zero_grad()清空变量梯度</li>\n<li>optim.SGD.step()执行优化</li>\n</ul>\n<p>官网实例：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.autograd <span class=\"keyword\">import</span> Variable</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Net</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        super(Net, self).__init__()</span><br><span class=\"line\">        <span class=\"comment\"># 1 input image channel, 6 output channels, 5x5 square convolution</span></span><br><span class=\"line\">        <span class=\"comment\"># kernel</span></span><br><span class=\"line\">        self.conv1 = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">6</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\">        self.conv2 = nn.Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\">        <span class=\"comment\"># an affine operation: y = Wx + b</span></span><br><span class=\"line\">        self.fc1 = nn.Linear(<span class=\"number\">16</span> * <span class=\"number\">5</span> * <span class=\"number\">5</span>, <span class=\"number\">120</span>)</span><br><span class=\"line\">        self.fc2 = nn.Linear(<span class=\"number\">120</span>, <span class=\"number\">84</span>)</span><br><span class=\"line\">        self.fc3 = nn.Linear(<span class=\"number\">84</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># Max pooling over a (2, 2) window</span></span><br><span class=\"line\">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class=\"number\">2</span>, <span class=\"number\">2</span>))</span><br><span class=\"line\">        <span class=\"comment\"># If the size is a square you can only specify a single number</span></span><br><span class=\"line\">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class=\"number\">2</span>)</span><br><span class=\"line\">        x = x.view(<span class=\"number\">-1</span>, self.num_flat_features(x))</span><br><span class=\"line\">        x = F.relu(self.fc1(x))</span><br><span class=\"line\">        x = F.relu(self.fc2(x))</span><br><span class=\"line\">        x = self.fc3(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">num_flat_features</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        size = x.size()[<span class=\"number\">1</span>:]  <span class=\"comment\"># all dimensions except the batch dimension</span></span><br><span class=\"line\">        num_features = <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> s <span class=\"keyword\">in</span> size:</span><br><span class=\"line\">            num_features *= s</span><br><span class=\"line\">        <span class=\"keyword\">return</span> num_features</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">net = Net()</span><br><span class=\"line\">print(net)</span><br></pre></td></tr></table></figure>\n<h2 id=\"4-分类器实战\"><a href=\"#4-分类器实战\" class=\"headerlink\" title=\"4. 分类器实战\"></a>4. 分类器实战</h2><ul>\n<li>torchvision.transforms：该模块提供多个转换器<ul>\n<li>transforms.Compose：将多个转换器串联起来</li>\n<li>transforms.ToTensor：Convert a <code>PIL Image</code> or <code>numpy.ndarray</code> to tensor.</li>\n<li>transforms.Normalize：(<em>mean, std</em>)，归一化图像三个维度（C，W，H）</li>\n</ul>\n</li>\n<li>torchvision.datasets<ul>\n<li>torchvision.datasets.CIFAR10：下载数据集</li>\n<li>torch.utils.data.DataLoader：加载数据集</li>\n</ul>\n</li>\n<li>torch.nn.CrossEntropyLoss<ul>\n<li>教程商误差？</li>\n</ul>\n</li>\n</ul>\n<p>—未完待续—</p>\n"}