{"summary":"<blockquote>\n<p>SVM是一种线性分类方法，数据量少的时候效果相对较好</p>\n<p>线性模型选择经验：<em>对于数据量大的线性方法，其实效果是差不多的，区别在于是否算法能够并行化，是否可以做到实时处理</em></p>\n</blockquote>\n<h2 id=\"1-逻辑回归回顾\"><a href=\"#1-逻辑回归回顾\" class=\"headerlink\" title=\"1 逻辑回归回顾\"></a>1 逻辑回归回顾</h2><h3 id=\"1-1-logistic数学模型\"><a href=\"#1-1-logistic数学模型\" class=\"headerlink\" title=\"1.1 logistic数学模型\"></a>1.1 logistic数学模型</h3><p>逻辑回归的模型是S型曲线模型（sigmoid），所以一般使用的模型函数是$h(\\theta^Tx) = \\frac 1 {1+e^{\\theta^Tx}}$</p>\n<p>有时可以对$\\theta^Tx$前面加个参数调整S形状。</p>\n<blockquote>\n<p>为什么叫logistic？</p>\n<p>摘自统计学习方法：</p>\n<p><img src=\"/2018/04/16/logist2SVM/logistic.png\" alt=\"logistic\"></p>\n</blockquote>\n<h3 id=\"1-2-损失函数\"><a href=\"#1-2-损失函数\" class=\"headerlink\" title=\"1.2 损失函数\"></a>1.2 损失函数</h3><p>为了衡量模型的好坏我们需要提出一个<strong>准则</strong>，即loss模型对样本预测结果的<strong>损失函数</strong></p>\n<p>对某个样本$(x_i, y_i)$预测结果的效果判断 :</p>\n<script type=\"math/tex; mode=display\">\nL(\\theta) = \\begin{cases} - log(h_{\\theta}(x_i)) & \\quad \\text{if } y_i = 1 \\\\\n-log(1- h_{\\theta}(x_i)) & \\quad \\text{if } y_i = 0 \\end{cases}</script><p>合并形式：</p>\n<script type=\"math/tex; mode=display\">\nL(\\theta) = - (y_i log(h_{\\theta}(x_i)) + (1-y_i) log(1-h_{\\theta}(x_i)))</script><blockquote>\n<p>损失函数为什么是取对数呢？</p>\n<ol>\n<li>从图形方面考虑是为了使得产生下面两种loss形状</li>\n</ol>\n<p><img src=\"/2018/04/16/logist2SVM/loss.png\" alt=\"loss\"></p>\n<ol>\n<li>符合最大熵模型中的一种极大似然估计的方法👇</li>\n</ol>\n</blockquote>\n<script type=\"math/tex; mode=display\">\nL(\\theta) = \\prod h(\\theta^Tx_i)^{y_i} h(\\theta^Tx_i)^{1-y_i} \\\\\n-log(L(\\theta)) = -(y_0*log(h(\\theta^Tx_0)) + (1-y_0)*log(h(\\theta^Tx_0))+….)</script><h2 id=\"2-SVM\"><a href=\"#2-SVM\" class=\"headerlink\" title=\"2. SVM\"></a>2. SVM</h2><h3 id=\"2-1-SVM的损失函数\"><a href=\"#2-1-SVM的损失函数\" class=\"headerlink\" title=\"2.1 SVM的损失函数\"></a>2.1 SVM的损失函数</h3><p>回顾logistic回归的损失函数的图像如下（这里应该是曲线的）</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th><img src=\"/2018/04/16/logist2SVM/loss1.png\" alt=\"loss1\"></th>\n<th><img src=\"/2018/04/16/logist2SVM/loss2.png\" alt=\"loss2\"></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><img src=\"/2018/04/16/logist2SVM/logistic-loss.png\" alt=\"SVM-loss\"></p>\n<p>当在SVM的情况下的loss是：</p>\n<p><img src=\"/2018/04/16/logist2SVM/SVM-loss.png\" alt=\"SVM-loss\"></p>\n<p>可以看到一个是loss的输入不同，一个是$h(\\theta^Tx)​$，一个是$\\theta^Tx​$；另一个是$cost_0, cost_1​$的不同，logistic回归的loss是log函数的平滑曲线，SVM的loss函数是直的折线</p>\n<blockquote>\n<p>折线表达式如何表示？$cost_1(z) = max{0, \\text {xxx}}$</p>\n<p>对loss形状的修改可以换成类似的其他函数，这样就可以开发成别的算法</p>\n</blockquote>\n<h3 id=\"2-2-SVM模型\"><a href=\"#2-2-SVM模型\" class=\"headerlink\" title=\"2.2 SVM模型\"></a>2.2 SVM模型</h3><blockquote>\n<p> SVM模型Loss表达式中各个式子的含义，推导出SVM模型的几何含义</p>\n</blockquote>\n<ul>\n<li><p>$min \\frac 1 2 \\theta^2$ 这是$\\theta$的模的平方</p>\n</li>\n<li><p>模型目标</p>\n<script type=\"math/tex; mode=display\">\n\\begin{cases}\\theta^Tx^{(i)} \\ge 1 & \\quad \\text {if } y^{(i)} = 1 \\\\\n\\theta^Tx^{(i)} \\le -1 & \\quad \\text {if } y^{(i)} = 0 \\end{cases}</script></li>\n</ul>\n<ul>\n<li>这里的$\\theta^Tx^{(i)}$ 可以看做是$x$向量在$\\theta$向量上面的投影与$\\theta$的模的乘积，而这里的$\\theta$就当作支持向量机里面超平面的法向量（超平面就是将样本安照标签分类的平面）</li>\n<li><p>模型解释：模型是一个超平面$\\theta$ ，对于在这个超平面两侧的平面$\\theta^Tx = 1, \\ \\theta^Tx = -1$，将数据分类到这两个平面两侧，所以要满足:</p>\n<ul>\n<li><script type=\"math/tex; mode=display\">\n\\begin{cases}\\theta^Tx^{(i)} \\ge 1 & \\quad \\text {if } y^{(i)} = 1 \\\\\n\\theta^Tx^{(i)} \\le -1 & \\quad \\text {if } y^{(i)} = 0 \\end{cases}</script></li>\n<li><p>而要求两个平面之间的距离尽量大，这两个平面之间的距离（margin）就是$\\frac 2 {|\\theta|}$ ，所以最大化$\\frac 2 {|\\theta|} $，可以转换为最小化$\\frac 1 2 \\theta^2$ ，即$min \\frac 1 2 \\theta^2$ </p>\n</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>通过上面的分析，我们可以看到SVM的几何含义就是：找出一个超平面使得将样本点按照标签分类，同时使得离超平面最近的样本点的距离最远（margin足够大）。</li>\n</ul>\n<h3 id=\"2-3-Insparable-Data-Hinge-Loss\"><a href=\"#2-3-Insparable-Data-Hinge-Loss\" class=\"headerlink\" title=\"2.3 Insparable Data - Hinge Loss\"></a>2.3 Insparable Data - Hinge Loss</h3><blockquote>\n<p>对于部分线性不可分的样本点，如何处理？</p>\n<p>：容许一定的错误，加入错误惩罚机制，使得尽量分开数据点</p>\n</blockquote>\n"}